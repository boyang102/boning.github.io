[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "My Projects",
    "section": "",
    "text": "This is Project 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is Project 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n\n\n\n\nBoning Yang\n\n\nMay 1, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nA Replication of Karlan and List (2007)\n\n\n\n\n\n\nBoning Yang\n\n\nApr 16, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMultinomial Logit Model\n\n\n\n\n\n\nBoning Yang\n\n\nMay 17, 2024\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/hw3.html",
    "href": "blog/hw3.html",
    "title": "Multinomial Logit Model",
    "section": "",
    "text": "This assignment expores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm."
  },
  {
    "objectID": "blog/hw3.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "href": "blog/hw3.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "title": "Multinomial Logit Model",
    "section": "1. Likelihood for the Multi-nomial Logit (MNL) Model",
    "text": "1. Likelihood for the Multi-nomial Logit (MNL) Model\nSuppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 3 products, then either \\(y=3\\) or \\(y=(0,0,1)\\) depending on how we want to represent it. Suppose also that we have a vector of data on each product \\(x_j\\) (eg, brand, price, etc.).\nWe model the consumer’s decision as the selection of the product that provides the most utility, and we’ll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 3 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^3e^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]"
  },
  {
    "objectID": "blog/hw3.html#simulate-conjoint-data",
    "href": "blog/hw3.html#simulate-conjoint-data",
    "title": "Multinomial Logit Model",
    "section": "2. Simulate Conjoint Data",
    "text": "2. Simulate Conjoint Data\nWe will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a “no choice” option; each simulated respondent must select one of the 3 alternatives.\nEach alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from $4 to $32 in increments of $4.\nThe part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer \\(i\\) for hypothethical streaming service \\(j\\) is\n\\[\nu_{ij} = (1 \\times Netflix_j) + (0.5 \\times Prime_j) + (-0.8*Ads_j) - 0.1\\times Price_j + \\varepsilon_{ij}\n\\]\nwhere the variables are binary indicators and \\(\\varepsilon\\) is Type 1 Extreme Value (ie, Gumble) distributed.\nThe following code provides the simulation of the conjoint data.\n\n\n\n\n\n\nNote\n\n\n\n\n\n\n# set seed for reproducibility\nset.seed(123)\n\n# define attributes\nbrand &lt;- c(\"N\", \"P\", \"H\") # Netflix, Prime, Hulu\nad &lt;- c(\"Yes\", \"No\")\nprice &lt;- seq(8, 32, by=4)\n\n# generate all possible profiles\nprofiles &lt;- expand.grid(\n    brand = brand,\n    ad = ad,\n    price = price\n)\nm &lt;- nrow(profiles)\n\n# assign part-worth utilities (true parameters)\nb_util &lt;- c(N = 1.0, P = 0.5, H = 0)\na_util &lt;- c(Yes = -0.8, No = 0.0)\np_util &lt;- function(p) -0.1 * p\n\n# number of respondents, choice tasks, and alternatives per task\nn_peeps &lt;- 100\nn_tasks &lt;- 10\nn_alts &lt;- 3\n\n# function to simulate one respondent’s data\nsim_one &lt;- function(id) {\n  \n    datlist &lt;- list()\n    \n    # loop over choice tasks\n    for (t in 1:n_tasks) {\n        \n        # randomly sample 3 alts (better practice would be to use a design)\n        dat &lt;- cbind(resp=id, task=t, profiles[sample(m, size=n_alts), ])\n        \n        # compute deterministic portion of utility\n        dat$v &lt;- b_util[dat$brand] + a_util[dat$ad] + p_util(dat$price) |&gt; round(10)\n        \n        # add Gumbel noise (Type I extreme value)\n        dat$e &lt;- -log(-log(runif(n_alts)))\n        dat$u &lt;- dat$v + dat$e\n        \n        # identify chosen alternative\n        dat$choice &lt;- as.integer(dat$u == max(dat$u))\n        \n        # store task\n        datlist[[t]] &lt;- dat\n    }\n    \n    # combine all tasks for one respondent\n    do.call(rbind, datlist)\n}\n\n# simulate data for all respondents\nconjoint_data &lt;- do.call(rbind, lapply(1:n_peeps, sim_one))\n\n# remove values unobservable to the researcher\nconjoint_data &lt;- conjoint_data[ , c(\"resp\", \"task\", \"brand\", \"ad\", \"price\", \"choice\")]\n\n# clean up\nrm(list=setdiff(ls(), \"conjoint_data\"))"
  },
  {
    "objectID": "blog/hw3.html#preparing-the-data-for-estimation",
    "href": "blog/hw3.html#preparing-the-data-for-estimation",
    "title": "Multinomial Logit Model",
    "section": "3. Preparing the Data for Estimation",
    "text": "3. Preparing the Data for Estimation\nThe “hard part” of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer \\(i\\), covariate \\(k\\), and product \\(j\\)) instead of the typical 2 dimensions for cross-sectional regression models (consumer \\(i\\) and covariate \\(k\\)). The fact that each task for each respondent has the same number of alternatives (3) helps. In addition, we need to convert the categorical variables for brand and ads into binary variables.\nFist, I am going to reshape and prepare the data:\n\nlibrary(dplyr)\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(readr)\n\nconjoint_data &lt;- read_csv(\"conjoint_data.csv\")\n\nRows: 3000 Columns: 6\n\n\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): brand, ad\ndbl (4): resp, task, choice, price\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n#from categorical var to binary indicators\nconjoint_data &lt;- conjoint_data |&gt;\n  mutate(brand_N = ifelse(brand == \"N\", 1, 0), brand_P = ifelse(brand == \"P\", 1, 0),\n    ad_yes  = ifelse(ad == \"Yes\", 1, 0) )\n\n#use task ID to uniquely identify each respondent-task combo\nconjoint_data &lt;- conjoint_data |&gt;mutate(task_id = paste(resp, task, sep = \"_\"))\nhead(conjoint_data)\n\n# A tibble: 6 × 10\n   resp  task choice brand ad    price brand_N brand_P ad_yes task_id\n  &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt; &lt;chr&gt;  \n1     1     1      1 N     Yes      28       1       0      1 1_1    \n2     1     1      0 H     Yes      16       0       0      1 1_1    \n3     1     1      0 P     Yes      16       0       1      1 1_1    \n4     1     2      0 N     Yes      32       1       0      1 1_2    \n5     1     2      1 P     Yes      16       0       1      1 1_2    \n6     1     2      0 N     Yes      24       1       0      1 1_2"
  },
  {
    "objectID": "blog/hw3.html#estimation-via-maximum-likelihood",
    "href": "blog/hw3.html#estimation-via-maximum-likelihood",
    "title": "Multinomial Logit Model",
    "section": "4. Estimation via Maximum Likelihood",
    "text": "4. Estimation via Maximum Likelihood\nI code up the log-likelihood function:\n\nX &lt;- as.matrix(conjoint_data[, c(\"brand_N\", \"brand_P\", \"ad_yes\", \"price\")]) #design matrix\ny &lt;- conjoint_data$choice\ntask_id &lt;- conjoint_data$task_id\n\nlog_likelihood &lt;- function(beta) {\n  beta &lt;- as.numeric(beta)\n  util &lt;- X %*% beta #linear utility\n  df &lt;- data.frame(task_id, util, y)\n  \n  ll &lt;- df |&gt;\n    group_by(task_id) |&gt;\n    mutate(prob = exp(util) / sum(exp(util))) |&gt;\n    ungroup() |&gt;\n    summarise(loglik = sum(y * log(prob))) |&gt;\n    pull(loglik)   #compute the log-likelihood\n   #negative log-likelihood for minimization purpose\n  return(-ll)}\n\nUse optim() in R to find the MLEs for the 4 parameters (\\(\\beta_\\text{netflix}\\), \\(\\beta_\\text{prime}\\), \\(\\beta_\\text{ads}\\), \\(\\beta_\\text{price}\\)), as well as their standard errors (from the Hessian). For each parameter construct a 95% confidence interval._\n\n# Initial guess for beta\nbeta_start &lt;- rep(0, 4)\n\nfit &lt;- optim(par = beta_start,fn = log_likelihood,hessian = TRUE,method = \"BFGS\")\nmle_estimates &lt;- fit$par\nvcov &lt;- solve(fit$hessian) #from Hessian\nse &lt;- sqrt(diag(vcov))\nci_lower &lt;- mle_estimates - 1.96 * se #95% CI\nci_upper &lt;- mle_estimates + 1.96 * se\nresults &lt;- data.frame(Parameter = c(\"beta_netflix\", \"beta_prime\", \"beta_ads\", \"beta_price\"),\n  Estimate = round(mle_estimates, 4),StdError = round(se, 4),CI_Lower = round(ci_lower, 4),\n  CI_Upper = round(ci_upper, 4))\n\nresults\n\n     Parameter Estimate StdError CI_Lower CI_Upper\n1 beta_netflix   0.9412   0.1110   0.7236   1.1588\n2   beta_prime   0.5016   0.1111   0.2839   0.7194\n3     beta_ads  -0.7320   0.0878  -0.9041  -0.5599\n4   beta_price  -0.0995   0.0063  -0.1119  -0.0871"
  },
  {
    "objectID": "blog/hw3.html#estimation-via-bayesian-methods",
    "href": "blog/hw3.html#estimation-via-bayesian-methods",
    "title": "Multinomial Logit Model",
    "section": "5. Estimation via Bayesian Methods",
    "text": "5. Estimation via Bayesian Methods\nNow, code up a metropolis-hasting MCMC sampler of the posterior distribution. Take 11,000 steps and throw away the first 1,000, retaining the subsequent 10,000. Use N(0,5) priors for the betas on the binary variables, and a N(0,1) prior for the price beta.\nInstead of calculating post=lik*prior, I work in the log-space and calculate log-post = log-lik + log-prior King Markov use a candidate distribution of a coin flip to decide whether to move left or right among his islands. Unlike King Markov, we have 4 dimensions (because we have 4 betas) and our dimensions are continuous. So, use a multivariate normal distribution to pospose the next location for the algorithm to move to. I recommend a MNV(mu, Sigma) where mu=c(0,0,0,0) and sigma has diagonal values c(0.05, 0.05, 0.05, 0.005) and zeros on the off-diagonal. Since this MVN has no covariances, I sample each dimension independently (so 4 univariate normals instead of 1 multivariate normal), where the first 3 univariate normals are N(0,0.05) and the last one if N(0,0.005).\n\nset.seed(1)\nlog_prior &lt;- function(beta) {dnorm(beta[1], 0, sqrt(5),log= TRUE) +dnorm(beta[2], 0, sqrt(5), log=TRUE) +\n    dnorm(beta[3], 0, sqrt(5), log= TRUE)+dnorm(beta[4], 0, 1, log = TRUE)}\n\n# log posterior=log likelihood+log prior\nlog_posterior &lt;- function(beta) {-log_likelihood(beta) + log_prior(beta)}\n\n# MCMC：\nn_steps &lt;- 11000\nbeta_dim &lt;- 4\nsamples &lt;- matrix(NA, nrow= n_steps, ncol= beta_dim)\nbeta_current &lt;- rep(0, beta_dim)\nlog_post_current &lt;- log_posterior(beta_current)\n\nproposal_sds &lt;- c(0.05, 0.05, 0.05, 0.005)\nfor (step in 1:n_steps) {\n  beta_proposal &lt;- beta_current + rnorm(beta_dim, 0, proposal_sds)\n  log_post_proposal &lt;- log_posterior(beta_proposal)\n\n  # Metropolis-Hastings acceptance\n  log_alpha &lt;- log_post_proposal - log_post_current\n  if (log(runif(1)) &lt; log_alpha) {\n    beta_current &lt;- beta_proposal\n    log_post_current &lt;- log_post_proposal}\n\n  samples[step, ] &lt;- beta_current}\nsamples_post &lt;- samples[1001:11000, ]\n\nNow, I make the trace plot of the algorithm, as well as the histogram of the posterior distribution.\n\nlibrary(ggplot2)\nposterior_df &lt;- data.frame(iteration = 1:nrow(samples_post),\n  beta_netflix = samples_post[, 1],\n  beta_prime   = samples_post[, 2],\n  beta_ads     = samples_post[, 3],\n  beta_price   = samples_post[, 4])\n\nggplot(posterior_df, aes(x = iteration, y = beta_ads)) +\n  geom_line(alpha = 0.5) +\n  labs(title = \"Trace Plot for beta_ads\", y = \"beta_ads\", x = \"Iteration\") +\n  theme_minimal()\n\n\n\n\n\n\n\nggplot(posterior_df, aes(x = beta_ads)) +\n  geom_histogram(bins = 50, fill = \"pink\", color = \"white\") +\n  labs(title = \"Posterior Distribution for beta_ads\", x = \"beta_ads\", y = \"Frequency\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nI report the 4 posterior means, standard deviations, and 95% credible intervals and compare them to the results from the Maximum Likelihood approach.\n\nbayes_summary &lt;- apply(samples_post, 2, function(x) {c(mean = mean(x), \n    sd = sd(x),ci_lower = quantile(x, 0.025),ci_upper = quantile(x, 0.975))})\nbayes_results &lt;- as.data.frame(t(bayes_summary))\ncolnames(bayes_results) &lt;- c(\"Mean\", \"SD\", \"CI_Lower\", \"CI_Upper\")\nbayes_results$Parameter &lt;- c(\"beta_netflix\", \"beta_prime\", \"beta_ads\", \"beta_price\")\nbayes_results &lt;- bayes_results[, c(\"Parameter\", \"Mean\", \"SD\", \"CI_Lower\", \"CI_Upper\")]\nbayes_results\n\n     Parameter        Mean          SD   CI_Lower    CI_Upper\n1 beta_netflix  0.94993283 0.111488896  0.7365745  1.16781812\n2   beta_prime  0.51015109 0.108916327  0.3012818  0.72232412\n3     beta_ads -0.73468901 0.087355354 -0.9133256 -0.56652134\n4   beta_price -0.09997589 0.006494711 -0.1130106 -0.08686107\n\n\nFrom the above two tables, you can see the posterior means and 95% credible intervals from the Bayesian MCMC are very close to the results from MLE. For the all four parameters, the differences are minimal. • For beta_netflix, the posterior mean = 0.950 vs MLE = 0.941 • For beta_prime, the posterior mean = 0.510 vs MLE = 0.502 • For beta_ads, the posterior mean = −0.735 vs MLE = −0.732 • For beta_price, the posterior mean = −0.100 vs MLE = −0.0995\nCredible intervals and MLE confidence intervals also mostly overlap, and this could suggest both approaches could yield consistent and reliable estimates."
  },
  {
    "objectID": "blog/hw3.html#discussion",
    "href": "blog/hw3.html#discussion",
    "title": "Multinomial Logit Model",
    "section": "6. Discussion",
    "text": "6. Discussion\nNow, suppose I did not simulate the data. What will the observation be about the parameter estimates? What does \\(\\beta_\\text{Netflix} &gt; \\beta_\\text{Prime}\\) mean? Does it make sense that \\(\\beta_\\text{price}\\) is negative?\nI we haven’t simulated the data, the results could still make sense:\nFirst, the fact that βNetflix &gt; βPrime indicates people tend to prefer Netflix over the Prime Video. This assume everything else is the same and it fits with what we might expect based on brand perception. Second, the negative βprice tells us that higher prices lower the likelihood of someone choosing a plan. It is very reasonable because people usually prefer cheaper options. Last, the strong negative coefficient for ads also confirms with our intuition sincemost people don’t like ads in their streaming experience.\nAt a high level, to simulate and estimate a hierarchical (random-parameter) model, I need to let each person have their own βs instead of using one set for everyone. In simulation, I draw each respondent’s β from a population distribution, such as a multivariate normal. In estimation, I would use a hierarchical model, such as hierarchical Bayes, to recover both the population-level trends and individual-level preferences.This lets me capture the real-world variation in tastes across people, which I think it is very useful."
  },
  {
    "objectID": "blog/hw2.html",
    "href": "blog/hw2.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\nLoad the data for this project:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nblueprinty &lt;- read_csv(\"blueprinty.csv\")\n\nRows: 1500 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): region\ndbl (3): patents, age, iscustomer\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(blueprinty)\n\n# A tibble: 6 × 4\n  patents region      age iscustomer\n    &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1       0 Midwest    32.5          0\n2       3 Southwest  37.5          0\n3       4 Northwest  27            1\n4       3 Northeast  24.5          0\n5       3 Southwest  37            0\n6       6 Northeast  29.5          1\n\n\nNow, compare histograms and means of number of patents by customer status.\n\n#means of number of patents by customer status\nblueprinty %&gt;%group_by(iscustomer) %&gt;%\n  summarise(mean_patents = mean(patents),sd_patents = sd(patents),n = n())\n\n# A tibble: 2 × 4\n  iscustomer mean_patents sd_patents     n\n       &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt; &lt;int&gt;\n1          0         3.47       2.23  1019\n2          1         4.13       2.55   481\n\n#histogram\nggplot(blueprinty, aes(x = patents, fill = factor(iscustomer))) +\n  geom_histogram(binwidth = 1, position = \"dodge\") +\n  scale_fill_manual(values = c(\"gray\", \"pink\"),labels = c(\"Non-Customer\", \"Customer\")) +\n  labs(title = \"Histogram of Patents by Customer Status\",\n    x = \"Number of Patents\",y = \"Count\",fill = \"Customer Status\") +theme_minimal()\n\n\n\n\n\n\n\n\nFrom above analysis, I observe that on average the Blueprinty customers have ~4.13 patents and for the non-customers, they have ~3.47 patents. By comparing this, we know that for the Blueprinty customers, they are more likely to have higher patent counts than the non-customers.The histogram also supports this hypothesis. I will use Poisson regression to better estimate it further.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n#compare avg ages\nblueprinty %&gt;%group_by(iscustomer) %&gt;%summarise(mean_age=mean(age),sd_age = sd(age))\n\n# A tibble: 2 × 3\n  iscustomer mean_age sd_age\n       &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n1          0     26.1   6.95\n2          1     26.9   7.81\n\n# age distribution\nggplot(blueprinty, aes(x = factor(iscustomer), y =age, fill= factor(iscustomer))) +\n  geom_boxplot() +scale_fill_manual(values = c(\"gray\", \"pink\"),labels = c(\"non customers\", \"customer\")) +labs(title = \"age vs customer status\",\n    x = \"status of customers\",y = \"age\",fill = \"customer status\")+theme_minimal()\n\n\n\n\n\n\n\n#regional location\nblueprinty %&gt;%count(region, iscustomer) %&gt;%group_by(region) %&gt;%\n  mutate(percent = n/sum(n)) %&gt;%ggplot(aes(x = region, y=percent, fill=factor(iscustomer))) +\n  geom_bar(stat = \"identity\") +scale_fill_manual(values = c(\"gray\", \"lightblue\"),\n                    labels = c(\"non customer\", \"customer\")) +\n  labs(title = \"regional distribution vs customer status\",x = \"regional location\",\n    y = \"proportion\",fill = \"status of customers\") +theme_minimal()\n\n\n\n\n\n\n\n\nCompare regions and ages by customer status, I notice that the mean age for non-customers is about 26.1 while the mean age for customers is about 26.9, which is slightly older here. The box plot shows the distribution of ages for the two customer status; for the customers (pink box), it has higher median and wider spread.\nFor the regional location part, the distribution here seems varies with customers’ status. There is a higher portion of customers located in Northeast than other areas, which indicates the customer adoption is not uniform and may not be random. Overall, both of them shows the importance of controlling age and region in the analysis to minimize the bias in the data.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nWrite down the mathematically the likelihood for_ \\(Y \\sim \\text{Poisson}(\\lambda)\\). Note that \\(f(Y|\\lambda) = e^{-\\lambda}\\lambda^Y/Y!\\).\n–For a single observation Y_i ~ Poisson(lambda_i), the probability mass function is: f(Y_i | lambda_i)=exp(-lambda_i)*lambda_i^Y_i/factorial(Y_i) –For multiple independent observations Y_1, …,Y_n with corresponding lambda_1, …, lambda_n, the joint likelihood is the product of individual likelihoods: L(lambda)=Π[ exp(-lambda_i)* lambda_i^Y_i/Y_i!] over i=1 to n –Taking the natural log of the likelihood gives the log-likelihood: logL(lambda)=Σ[-lambda_i+ Y_i*log(lambda_i)-log(Y_i!)] over i = 1 to n –log-likelihood function is to take lambda and observed Y\nCode the likelihood (or log-likelihood) function for the Poisson model. This is a function of lambda and Y. For example:\npoisson_loglikelihood &lt;- function(lambda, Y){\n   sum(-lambda+Y*log(lambda)-lgamma(Y+1))}\nUse the function to plot lambda on the horizontal axis and the likelihood (or log-likelihood) on the vertical axis for a range of lambdas (use the observed number of patents as the input for Y).\n\npoisson_loglikelihood &lt;- function(lambda, Y) {sum(-lambda+Y*log(lambda) - lgamma(Y+1))}\n\n#use one observed Y value\nY_eg &lt;- blueprinty$patents[10]\n#lambda values sequence\nlambs &lt;- seq(0.1, 10, by = 0.1)\n#find log-likelihood for each lambda\nloglik_vals &lt;- sapply(lambs, function(l) poisson_loglikelihood(l,Y_eg))\n\n# plot log-likelihood curve\nplot(lambs, loglik_vals, type = \"l\",\n     col = \"blue\", xlab = expression(lambda),\n     ylab = \"Log Likelihood\",\n     main = paste(\"Log Likelihood at Y =\", Y_eg))\nabline(v=Y_eg, col= \"pink\")\n\n\n\n\n\n\n\n\nLet’s ttake the first derivative of log-likelihood, set it equal to zero and solve for lambda. I find lambda_mle is Ybar, which “feels right” because the mean of a Poisson distribution is lambda.\nSo here, I begin with the log-likelihood function for n i.i.d. observations from a Poisson distribution with the common mean , I get:\nL() = _{i=1}^n ( -+ Y_i - Y_i! )\nI then drop the term Y_i! because it does not depend on .Now, it simplify to the following equation:\nL() = -n+ _{i=1}^n Y_i\nTaking the first derivative with respect to :\n-n + _{i=1}^n Y_i\nNow, let the derivative to be zero and I want to use it to find the maximum value.Then I find:\n= _{i=1}^n Y_i = {Y}\nNow, find the MLE of lambada by maximizing the log-likelihood function using R’s optim().\n\n#negated log-likelihood function for minimization\nneg_ll &lt;- function(lambda, Y) {\n#avoid log(0)\n  if (lambda &lt;= 0) return(Inf)\n  return(-sum(-lambda + Y * log(lambda) - lgamma(Y + 1)))}\nY_data &lt;- blueprinty$patents\n#minimize the negative log-likelihood\nmle_result &lt;- optim(par = 1, fn = neg_ll,Y=Y_data,method=\"BFGS\",hessian = TRUE)\nlambda_mle &lt;- mle_result$par #estimate lambda\nlambda_mle\n\n[1] 3.68467\n\nmean(Y_data)\n\n[1] 3.684667\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n# update log-likelihood for Poisson\npoisson_regression_likelihood &lt;- function(beta, Y, X) {\n  # X:covariates matrix, beta:vector of coefficients\n  lambda &lt;- exp(X %*% beta) #inverse\n  loglik &lt;- sum(-lambda+Y*log(lambda)-lgamma(Y+1))\n  return(-loglik)}\n\n\nblueprinty &lt;- blueprinty %&gt;%mutate(age2=age^2)\nX &lt;- model.matrix(~age + age2+region+iscustomer, data=blueprinty)\nY &lt;- blueprinty$patents\n\n#let initial parameter as 0\nbeta0 &lt;- rep(0, ncol(X))\nresult &lt;- optim(par=beta0,fn = poisson_regression_likelihood,Y=Y,X=X,\n                method = \"BFGS\",hessian = TRUE)\n\nbeta_hat &lt;- result$par\n#calculate the standard errors by inverse hessian\nhessian_mat &lt;- result$hessian\nse_beta &lt;- sqrt(diag(solve(hessian_mat)))\n\ncoef_table &lt;- data.frame(Term = colnames(X),Estimate = beta_hat,Std_Error = se_beta)\nknitr::kable(coef_table, digits = 4,\n             caption = \"poisson coefficient estimates&standard errors\")\n\n\npoisson coefficient estimates&standard errors\n\n\nTerm\nEstimate\nStd_Error\n\n\n\n\n(Intercept)\n-0.1257\n0.1122\n\n\nage\n0.1158\n0.0064\n\n\nage2\n-0.0022\n0.0001\n\n\nregionNortheast\n-0.0246\n0.0434\n\n\nregionNorthwest\n-0.0348\n0.0529\n\n\nregionSouth\n-0.0054\n0.0524\n\n\nregionSouthwest\n-0.0378\n0.0472\n\n\niscustomer\n0.0607\n0.0321\n\n\n\n\n\nFrom the above table, the age coefficient is positive since it says 0.1158, while the age square coefficient is negative at –0.0022. This suggests a concave relationship between the firm age and patent count and it means the older firms tend to produce more patents up to a point, after which the effect disappear. The iscustomer coefficient is positive at 0.0607. It indicates that firms using Blueprinty’s software may have a higher expected number of patents. In addition, the regional effects are all negative compared to the omitted base region. However, none of these appear statistically significant at conventional levels.\nCheck my results using R’s glm() function:\n\nglm_model &lt;- glm(patents ~ age+I(age^2)+region+iscustomer,data=blueprinty,family=poisson())\nsummary(glm_model)\n\n\nCall:\nglm(formula = patents ~ age + I(age^2) + region + iscustomer, \n    family = poisson(), data = blueprinty)\n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)     -0.508920   0.183179  -2.778  0.00546 ** \nage              0.148619   0.013869  10.716  &lt; 2e-16 ***\nI(age^2)        -0.002971   0.000258 -11.513  &lt; 2e-16 ***\nregionNortheast  0.029170   0.043625   0.669  0.50372    \nregionNorthwest -0.017574   0.053781  -0.327  0.74383    \nregionSouth      0.056561   0.052662   1.074  0.28281    \nregionSouthwest  0.050576   0.047198   1.072  0.28391    \niscustomer       0.207591   0.030895   6.719 1.83e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 2362.5  on 1499  degrees of freedom\nResidual deviance: 2143.3  on 1492  degrees of freedom\nAIC: 6532.1\n\nNumber of Fisher Scoring iterations: 5\n\n\n\n#combine glm and optim output all together in a table\ncompare &lt;- data.frame(\n  Term = names(coef(glm_model)),\n  Estimate_glm = coef(glm_model),\n  Estimate_optim = coef_table$Estimate,\n  SE_glm = summary(glm_model)$coefficients[, \"Std. Error\"],\n  SE_optim = coef_table$Std_Error)\n\nknitr::kable(compare, digits=4, caption=\"comparison of glm() and optim() estimates\")\n\n\ncomparison of glm() and optim() estimates\n\n\n\n\n\n\n\n\n\n\n\nTerm\nEstimate_glm\nEstimate_optim\nSE_glm\nSE_optim\n\n\n\n\n(Intercept)\n(Intercept)\n-0.5089\n-0.1257\n0.1832\n0.1122\n\n\nage\nage\n0.1486\n0.1158\n0.0139\n0.0064\n\n\nI(age^2)\nI(age^2)\n-0.0030\n-0.0022\n0.0003\n0.0001\n\n\nregionNortheast\nregionNortheast\n0.0292\n-0.0246\n0.0436\n0.0434\n\n\nregionNorthwest\nregionNorthwest\n-0.0176\n-0.0348\n0.0538\n0.0529\n\n\nregionSouth\nregionSouth\n0.0566\n-0.0054\n0.0527\n0.0524\n\n\nregionSouthwest\nregionSouthwest\n0.0506\n-0.0378\n0.0472\n0.0472\n\n\niscustomer\niscustomer\n0.2076\n0.0607\n0.0309\n0.0321\n\n\n\n\n\nFrom the above results, it verifies the estimates from glm() and optim() are mostly consistent,and this further confirms the right of the MLE.\nThe table demonstrates that both age and age square show a clear inverted-U relationship with the patent counts. The iscustomer has a positive effect which suggests Blueprinty users file more patents, although the size differs slightly between each method. Third, the region effects are small and not statistically significant at the level, while the intercept varies more, mostly may due to optimizer sensitivity. Overall, the firm age and customer status are the most influential predictors.\n\n# counterfactual covariate matrices\nX_0 &lt;- X\nX_0[, \"iscustomer\"] &lt;- 0\nX_1 &lt;- X\nX_1[, \"iscustomer\"] &lt;- 1\n#predict lambda_i for both scenarios\nlambda_0 &lt;- exp(X_0 %*% beta_hat)\nlambda_1 &lt;- exp(X_1 %*% beta_hat)\ndiff &lt;- lambda_1 - lambda_0 #average treatment effect\nate &lt;- mean(diff)\nate\n\n[1] 0.2178843\n\n\nBased on above results, the firms that use Blueprinty’s software are predicted to receive ~0.22 more patents on average than similar firms which do not use the software. This estimated effect is small, but it can support the claim that Blueprinty may help improve the patenting outcomes."
  },
  {
    "objectID": "blog/hw2.html#blueprinty-case-study",
    "href": "blog/hw2.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\nLoad the data for this project:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nblueprinty &lt;- read_csv(\"blueprinty.csv\")\n\nRows: 1500 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): region\ndbl (3): patents, age, iscustomer\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(blueprinty)\n\n# A tibble: 6 × 4\n  patents region      age iscustomer\n    &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1       0 Midwest    32.5          0\n2       3 Southwest  37.5          0\n3       4 Northwest  27            1\n4       3 Northeast  24.5          0\n5       3 Southwest  37            0\n6       6 Northeast  29.5          1\n\n\nNow, compare histograms and means of number of patents by customer status.\n\n#means of number of patents by customer status\nblueprinty %&gt;%group_by(iscustomer) %&gt;%\n  summarise(mean_patents = mean(patents),sd_patents = sd(patents),n = n())\n\n# A tibble: 2 × 4\n  iscustomer mean_patents sd_patents     n\n       &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt; &lt;int&gt;\n1          0         3.47       2.23  1019\n2          1         4.13       2.55   481\n\n#histogram\nggplot(blueprinty, aes(x = patents, fill = factor(iscustomer))) +\n  geom_histogram(binwidth = 1, position = \"dodge\") +\n  scale_fill_manual(values = c(\"gray\", \"pink\"),labels = c(\"Non-Customer\", \"Customer\")) +\n  labs(title = \"Histogram of Patents by Customer Status\",\n    x = \"Number of Patents\",y = \"Count\",fill = \"Customer Status\") +theme_minimal()\n\n\n\n\n\n\n\n\nFrom above analysis, I observe that on average the Blueprinty customers have ~4.13 patents and for the non-customers, they have ~3.47 patents. By comparing this, we know that for the Blueprinty customers, they are more likely to have higher patent counts than the non-customers.The histogram also supports this hypothesis. I will use Poisson regression to better estimate it further.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n#compare avg ages\nblueprinty %&gt;%group_by(iscustomer) %&gt;%summarise(mean_age=mean(age),sd_age = sd(age))\n\n# A tibble: 2 × 3\n  iscustomer mean_age sd_age\n       &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n1          0     26.1   6.95\n2          1     26.9   7.81\n\n# age distribution\nggplot(blueprinty, aes(x = factor(iscustomer), y =age, fill= factor(iscustomer))) +\n  geom_boxplot() +scale_fill_manual(values = c(\"gray\", \"pink\"),labels = c(\"non customers\", \"customer\")) +labs(title = \"age vs customer status\",\n    x = \"status of customers\",y = \"age\",fill = \"customer status\")+theme_minimal()\n\n\n\n\n\n\n\n#regional location\nblueprinty %&gt;%count(region, iscustomer) %&gt;%group_by(region) %&gt;%\n  mutate(percent = n/sum(n)) %&gt;%ggplot(aes(x = region, y=percent, fill=factor(iscustomer))) +\n  geom_bar(stat = \"identity\") +scale_fill_manual(values = c(\"gray\", \"lightblue\"),\n                    labels = c(\"non customer\", \"customer\")) +\n  labs(title = \"regional distribution vs customer status\",x = \"regional location\",\n    y = \"proportion\",fill = \"status of customers\") +theme_minimal()\n\n\n\n\n\n\n\n\nCompare regions and ages by customer status, I notice that the mean age for non-customers is about 26.1 while the mean age for customers is about 26.9, which is slightly older here. The box plot shows the distribution of ages for the two customer status; for the customers (pink box), it has higher median and wider spread.\nFor the regional location part, the distribution here seems varies with customers’ status. There is a higher portion of customers located in Northeast than other areas, which indicates the customer adoption is not uniform and may not be random. Overall, both of them shows the importance of controlling age and region in the analysis to minimize the bias in the data.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nWrite down the mathematically the likelihood for_ \\(Y \\sim \\text{Poisson}(\\lambda)\\). Note that \\(f(Y|\\lambda) = e^{-\\lambda}\\lambda^Y/Y!\\).\n–For a single observation Y_i ~ Poisson(lambda_i), the probability mass function is: f(Y_i | lambda_i)=exp(-lambda_i)*lambda_i^Y_i/factorial(Y_i) –For multiple independent observations Y_1, …,Y_n with corresponding lambda_1, …, lambda_n, the joint likelihood is the product of individual likelihoods: L(lambda)=Π[ exp(-lambda_i)* lambda_i^Y_i/Y_i!] over i=1 to n –Taking the natural log of the likelihood gives the log-likelihood: logL(lambda)=Σ[-lambda_i+ Y_i*log(lambda_i)-log(Y_i!)] over i = 1 to n –log-likelihood function is to take lambda and observed Y\nCode the likelihood (or log-likelihood) function for the Poisson model. This is a function of lambda and Y. For example:\npoisson_loglikelihood &lt;- function(lambda, Y){\n   sum(-lambda+Y*log(lambda)-lgamma(Y+1))}\nUse the function to plot lambda on the horizontal axis and the likelihood (or log-likelihood) on the vertical axis for a range of lambdas (use the observed number of patents as the input for Y).\n\npoisson_loglikelihood &lt;- function(lambda, Y) {sum(-lambda+Y*log(lambda) - lgamma(Y+1))}\n\n#use one observed Y value\nY_eg &lt;- blueprinty$patents[10]\n#lambda values sequence\nlambs &lt;- seq(0.1, 10, by = 0.1)\n#find log-likelihood for each lambda\nloglik_vals &lt;- sapply(lambs, function(l) poisson_loglikelihood(l,Y_eg))\n\n# plot log-likelihood curve\nplot(lambs, loglik_vals, type = \"l\",\n     col = \"blue\", xlab = expression(lambda),\n     ylab = \"Log Likelihood\",\n     main = paste(\"Log Likelihood at Y =\", Y_eg))\nabline(v=Y_eg, col= \"pink\")\n\n\n\n\n\n\n\n\nLet’s ttake the first derivative of log-likelihood, set it equal to zero and solve for lambda. I find lambda_mle is Ybar, which “feels right” because the mean of a Poisson distribution is lambda.\nSo here, I begin with the log-likelihood function for n i.i.d. observations from a Poisson distribution with the common mean , I get:\nL() = _{i=1}^n ( -+ Y_i - Y_i! )\nI then drop the term Y_i! because it does not depend on .Now, it simplify to the following equation:\nL() = -n+ _{i=1}^n Y_i\nTaking the first derivative with respect to :\n-n + _{i=1}^n Y_i\nNow, let the derivative to be zero and I want to use it to find the maximum value.Then I find:\n= _{i=1}^n Y_i = {Y}\nNow, find the MLE of lambada by maximizing the log-likelihood function using R’s optim().\n\n#negated log-likelihood function for minimization\nneg_ll &lt;- function(lambda, Y) {\n#avoid log(0)\n  if (lambda &lt;= 0) return(Inf)\n  return(-sum(-lambda + Y * log(lambda) - lgamma(Y + 1)))}\nY_data &lt;- blueprinty$patents\n#minimize the negative log-likelihood\nmle_result &lt;- optim(par = 1, fn = neg_ll,Y=Y_data,method=\"BFGS\",hessian = TRUE)\nlambda_mle &lt;- mle_result$par #estimate lambda\nlambda_mle\n\n[1] 3.68467\n\nmean(Y_data)\n\n[1] 3.684667\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n# update log-likelihood for Poisson\npoisson_regression_likelihood &lt;- function(beta, Y, X) {\n  # X:covariates matrix, beta:vector of coefficients\n  lambda &lt;- exp(X %*% beta) #inverse\n  loglik &lt;- sum(-lambda+Y*log(lambda)-lgamma(Y+1))\n  return(-loglik)}\n\n\nblueprinty &lt;- blueprinty %&gt;%mutate(age2=age^2)\nX &lt;- model.matrix(~age + age2+region+iscustomer, data=blueprinty)\nY &lt;- blueprinty$patents\n\n#let initial parameter as 0\nbeta0 &lt;- rep(0, ncol(X))\nresult &lt;- optim(par=beta0,fn = poisson_regression_likelihood,Y=Y,X=X,\n                method = \"BFGS\",hessian = TRUE)\n\nbeta_hat &lt;- result$par\n#calculate the standard errors by inverse hessian\nhessian_mat &lt;- result$hessian\nse_beta &lt;- sqrt(diag(solve(hessian_mat)))\n\ncoef_table &lt;- data.frame(Term = colnames(X),Estimate = beta_hat,Std_Error = se_beta)\nknitr::kable(coef_table, digits = 4,\n             caption = \"poisson coefficient estimates&standard errors\")\n\n\npoisson coefficient estimates&standard errors\n\n\nTerm\nEstimate\nStd_Error\n\n\n\n\n(Intercept)\n-0.1257\n0.1122\n\n\nage\n0.1158\n0.0064\n\n\nage2\n-0.0022\n0.0001\n\n\nregionNortheast\n-0.0246\n0.0434\n\n\nregionNorthwest\n-0.0348\n0.0529\n\n\nregionSouth\n-0.0054\n0.0524\n\n\nregionSouthwest\n-0.0378\n0.0472\n\n\niscustomer\n0.0607\n0.0321\n\n\n\n\n\nFrom the above table, the age coefficient is positive since it says 0.1158, while the age square coefficient is negative at –0.0022. This suggests a concave relationship between the firm age and patent count and it means the older firms tend to produce more patents up to a point, after which the effect disappear. The iscustomer coefficient is positive at 0.0607. It indicates that firms using Blueprinty’s software may have a higher expected number of patents. In addition, the regional effects are all negative compared to the omitted base region. However, none of these appear statistically significant at conventional levels.\nCheck my results using R’s glm() function:\n\nglm_model &lt;- glm(patents ~ age+I(age^2)+region+iscustomer,data=blueprinty,family=poisson())\nsummary(glm_model)\n\n\nCall:\nglm(formula = patents ~ age + I(age^2) + region + iscustomer, \n    family = poisson(), data = blueprinty)\n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)     -0.508920   0.183179  -2.778  0.00546 ** \nage              0.148619   0.013869  10.716  &lt; 2e-16 ***\nI(age^2)        -0.002971   0.000258 -11.513  &lt; 2e-16 ***\nregionNortheast  0.029170   0.043625   0.669  0.50372    \nregionNorthwest -0.017574   0.053781  -0.327  0.74383    \nregionSouth      0.056561   0.052662   1.074  0.28281    \nregionSouthwest  0.050576   0.047198   1.072  0.28391    \niscustomer       0.207591   0.030895   6.719 1.83e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 2362.5  on 1499  degrees of freedom\nResidual deviance: 2143.3  on 1492  degrees of freedom\nAIC: 6532.1\n\nNumber of Fisher Scoring iterations: 5\n\n\n\n#combine glm and optim output all together in a table\ncompare &lt;- data.frame(\n  Term = names(coef(glm_model)),\n  Estimate_glm = coef(glm_model),\n  Estimate_optim = coef_table$Estimate,\n  SE_glm = summary(glm_model)$coefficients[, \"Std. Error\"],\n  SE_optim = coef_table$Std_Error)\n\nknitr::kable(compare, digits=4, caption=\"comparison of glm() and optim() estimates\")\n\n\ncomparison of glm() and optim() estimates\n\n\n\n\n\n\n\n\n\n\n\nTerm\nEstimate_glm\nEstimate_optim\nSE_glm\nSE_optim\n\n\n\n\n(Intercept)\n(Intercept)\n-0.5089\n-0.1257\n0.1832\n0.1122\n\n\nage\nage\n0.1486\n0.1158\n0.0139\n0.0064\n\n\nI(age^2)\nI(age^2)\n-0.0030\n-0.0022\n0.0003\n0.0001\n\n\nregionNortheast\nregionNortheast\n0.0292\n-0.0246\n0.0436\n0.0434\n\n\nregionNorthwest\nregionNorthwest\n-0.0176\n-0.0348\n0.0538\n0.0529\n\n\nregionSouth\nregionSouth\n0.0566\n-0.0054\n0.0527\n0.0524\n\n\nregionSouthwest\nregionSouthwest\n0.0506\n-0.0378\n0.0472\n0.0472\n\n\niscustomer\niscustomer\n0.2076\n0.0607\n0.0309\n0.0321\n\n\n\n\n\nFrom the above results, it verifies the estimates from glm() and optim() are mostly consistent,and this further confirms the right of the MLE.\nThe table demonstrates that both age and age square show a clear inverted-U relationship with the patent counts. The iscustomer has a positive effect which suggests Blueprinty users file more patents, although the size differs slightly between each method. Third, the region effects are small and not statistically significant at the level, while the intercept varies more, mostly may due to optimizer sensitivity. Overall, the firm age and customer status are the most influential predictors.\n\n# counterfactual covariate matrices\nX_0 &lt;- X\nX_0[, \"iscustomer\"] &lt;- 0\nX_1 &lt;- X\nX_1[, \"iscustomer\"] &lt;- 1\n#predict lambda_i for both scenarios\nlambda_0 &lt;- exp(X_0 %*% beta_hat)\nlambda_1 &lt;- exp(X_1 %*% beta_hat)\ndiff &lt;- lambda_1 - lambda_0 #average treatment effect\nate &lt;- mean(diff)\nate\n\n[1] 0.2178843\n\n\nBased on above results, the firms that use Blueprinty’s software are predicted to receive ~0.22 more patents on average than similar firms which do not use the software. This estimated effect is small, but it can support the claim that Blueprinty may help improve the patenting outcomes."
  },
  {
    "objectID": "blog/hw2.html#airbnb-case-study",
    "href": "blog/hw2.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\ntodo: Assume the number of reviews is a good proxy for the number of bookings. Perform some exploratory data analysis to get a feel for the data, handle or drop observations with missing values on relevant variables, build one or more models (e.g., a poisson regression model for the number of bookings as proxied by the number of reviews), and interpret model coefficients to describe variation in the number of reviews as a function of the variables provided.\n\n#exploratory data analysis\nairbnb &lt;- read_csv(\"airbnb.csv\")\n\nNew names:\nRows: 40628 Columns: 14\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(3): last_scraped, host_since, room_type dbl (10): ...1, id, days, bathrooms,\nbedrooms, price, number_of_reviews, rev... lgl (1): instant_bookable\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\n#find relevant variables and remove missing values\nairbnb_new &lt;- airbnb %&gt;%\n  select(price, bathrooms, bedrooms, number_of_reviews, review_scores_cleanliness, review_scores_location, review_scores_value, instant_bookable, room_type) %&gt;%\n  drop_na()\nairbnb_new &lt;- airbnb_new %&gt;%\n  mutate(instant_bookable = factor(instant_bookable), room_type = factor(room_type))\nsummary(airbnb_new)\n\n     price           bathrooms        bedrooms      number_of_reviews\n Min.   :   10.0   Min.   :0.000   Min.   : 0.000   Min.   :  1.00   \n 1st Qu.:   70.0   1st Qu.:1.000   1st Qu.: 1.000   1st Qu.:  3.00   \n Median :  103.0   Median :1.000   Median : 1.000   Median :  8.00   \n Mean   :  140.2   Mean   :1.122   Mean   : 1.151   Mean   : 21.17   \n 3rd Qu.:  169.0   3rd Qu.:1.000   3rd Qu.: 1.000   3rd Qu.: 26.00   \n Max.   :10000.0   Max.   :6.000   Max.   :10.000   Max.   :421.00   \n review_scores_cleanliness review_scores_location review_scores_value\n Min.   : 2.000            Min.   : 2.000         Min.   : 2.000     \n 1st Qu.: 9.000            1st Qu.: 9.000         1st Qu.: 9.000     \n Median :10.000            Median :10.000         Median :10.000     \n Mean   : 9.202            Mean   : 9.415         Mean   : 9.334     \n 3rd Qu.:10.000            3rd Qu.:10.000         3rd Qu.:10.000     \n Max.   :10.000            Max.   :10.000         Max.   :10.000     \n instant_bookable           room_type    \n FALSE:24243      Entire home/apt:15543  \n TRUE : 5917      Private room   :13773  \n                  Shared room    :  844  \n                                         \n                                         \n                                         \n\n\n\n#explore data\nhist(airbnb_new$number_of_reviews, breaks=50, col=\"pink\", main= \"Number of Reviews\", xlab=\"Number of Reviews\")\n\n\n\n\n\n\n\nplot(airbnb_new$price, airbnb_new$number_of_reviews,xlab = \"Price\", ylab = \"Number of Reviews\",\n     main = \"Price vs Number of Reviews\", col= \"lightblue\")\n\n\n\n\n\n\n\nplot(airbnb_new$price, log1p(airbnb_new$number_of_reviews),xlab = \"Price\", ylab = \"Log(Number of Reviews+1)\",main = \"Price vs Log Reviews\",col= \"lightgreen\")\n\n\n\n\n\n\n\n\n\nmodel_airbnb &lt;- glm(number_of_reviews ~ price + bathrooms + bedrooms + \n                    review_scores_cleanliness + review_scores_location + \n                    review_scores_value + instant_bookable + room_type,\n                    data = airbnb_new, family = poisson())\n\nsummary(model_airbnb)\n\n\nCall:\nglm(formula = number_of_reviews ~ price + bathrooms + bedrooms + \n    review_scores_cleanliness + review_scores_location + review_scores_value + \n    instant_bookable + room_type, family = poisson(), data = airbnb_new)\n\nCoefficients:\n                            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                3.572e+00  1.600e-02 223.215  &lt; 2e-16 ***\nprice                     -1.436e-05  8.303e-06  -1.729   0.0838 .  \nbathrooms                 -1.240e-01  3.747e-03 -33.091  &lt; 2e-16 ***\nbedrooms                   7.494e-02  1.988e-03  37.698  &lt; 2e-16 ***\nreview_scores_cleanliness  1.132e-01  1.493e-03  75.821  &lt; 2e-16 ***\nreview_scores_location    -7.680e-02  1.607e-03 -47.796  &lt; 2e-16 ***\nreview_scores_value       -9.153e-02  1.798e-03 -50.902  &lt; 2e-16 ***\ninstant_bookableTRUE       3.344e-01  2.889e-03 115.748  &lt; 2e-16 ***\nroom_typePrivate room     -1.453e-02  2.737e-03  -5.310 1.09e-07 ***\nroom_typeShared room      -2.519e-01  8.618e-03 -29.229  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 961626  on 30159  degrees of freedom\nResidual deviance: 936528  on 30150  degrees of freedom\nAIC: 1058014\n\nNumber of Fisher Scoring iterations: 6\n\n\nFrom the above results, I notice that listings with more bedrooms, higher cleanliness scores, and instant bookable status tend to have more reviews. The bathrooms and higher prices are related to fewer reviews. In addition, for the shared rooms, it has fewer reviews than the entire homes, but the private rooms show negative effect. The cleanliness is the most influential review score here; location and value have unexpected negative coefficients; this may be due to bias existing in the sample. To sum up, from the exploration of the data, it shows the convenience, cleanliness, and room type are key drivers of booking activity."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Boning Yang",
    "section": "",
    "text": "Here’s a paragraph about me :)"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "blog/project1/index.html",
    "href": "blog/project1/index.html",
    "title": "This is Project 1",
    "section": "",
    "text": "I cleaned some data\n\n\n\nI analyzed the data\n#| message: false\nlibrary(tidyverse) mtcars |&gt; ggplot(aes(x = wt, y = mpg)) + geom_point()"
  },
  {
    "objectID": "blog/project1/index.html#section-1-data",
    "href": "blog/project1/index.html#section-1-data",
    "title": "This is Project 1",
    "section": "",
    "text": "I cleaned some data"
  },
  {
    "objectID": "blog/project1/index.html#section-2-analysis",
    "href": "blog/project1/index.html#section-2-analysis",
    "title": "This is Project 1",
    "section": "",
    "text": "I analyzed the data\n#| message: false\nlibrary(tidyverse) mtcars |&gt; ggplot(aes(x = wt, y = mpg)) + geom_point()"
  },
  {
    "objectID": "blog/MGT495_HW1_.html",
    "href": "blog/MGT495_HW1_.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a large-scaled field experiment to investigate whether the matching donations could affect charitable giving behavior. They sent out 50,000 fundraising letters to potential donors and this experiment contains 50,083 participants.\nEach participant was randomly assigned to a control group or a treatment group.For the control group, the participant would receive a 4-pages fundraising letter, while for the treatment group, participants received a very similar letter but it contains an announcement of matching grant, which is promised by the lead donor in order to match the recipient’s donation. Within the treatment group, there are three further treatments: first, researchers add the matching ratio, which contains 1:1, 1:2, or 1:3 match. It indicates the relationship between donation and match values; second, the highest total matching funds is 25,000, 50,000, and 100,000 dollars; third, there is a suggested donation amount on each letter, including 1x,1.25x, or 1.5x.\nThe overall purpose of this experiment design is to see whether the matching contribute to the increasing in donation overall and whether the larger match ratio is more effective or not. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThis project seeks to replicate their results using the original data."
  },
  {
    "objectID": "blog/MGT495_HW1_.html#introduction",
    "href": "blog/MGT495_HW1_.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a large-scaled field experiment to investigate whether the matching donations could affect charitable giving behavior. They sent out 50,000 fundraising letters to potential donors and this experiment contains 50,083 participants.\nEach participant was randomly assigned to a control group or a treatment group.For the control group, the participant would receive a 4-pages fundraising letter, while for the treatment group, participants received a very similar letter but it contains an announcement of matching grant, which is promised by the lead donor in order to match the recipient’s donation. Within the treatment group, there are three further treatments: first, researchers add the matching ratio, which contains 1:1, 1:2, or 1:3 match. It indicates the relationship between donation and match values; second, the highest total matching funds is 25,000, 50,000, and 100,000 dollars; third, there is a suggested donation amount on each letter, including 1x,1.25x, or 1.5x.\nThe overall purpose of this experiment design is to see whether the matching contribute to the increasing in donation overall and whether the larger match ratio is more effective or not. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThis project seeks to replicate their results using the original data."
  },
  {
    "objectID": "blog/MGT495_HW1_.html#data",
    "href": "blog/MGT495_HW1_.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\nI conduct t-tests and simple linear regression models on three selected variables: mrm2, which is the months since last donation; hpa, which is the highest previous contribution; and female, which is a gender indicator with value 1 equal to female. The formula is like what I learned from the class.\nT-test shows as below:\n\n# conduct t-test for selected varaibles above\nt.test(mrm2 ~ treatment, data = data)\n\n\n    Welch Two Sample t-test\n\ndata:  mrm2 by treatment\nt = -0.11953, df = 33394, p-value = 0.9049\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -0.2381015  0.2107298\nsample estimates:\nmean in group 0 mean in group 1 \n       12.99814        13.01183 \n\nt.test(hpa ~ treatment, data = data)\n\n\n    Welch Two Sample t-test\n\ndata:  hpa by treatment\nt = -0.97043, df = 35914, p-value = 0.3318\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -1.9238107  0.6496601\nsample estimates:\nmean in group 0 mean in group 1 \n       58.96017        59.59724 \n\nt.test(female ~ treatment, data = data)\n\n\n    Welch Two Sample t-test\n\ndata:  female by treatment\nt = 1.7535, df = 32451, p-value = 0.07952\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -0.0008888548  0.0159826921\nsample estimates:\nmean in group 0 mean in group 1 \n      0.2826978       0.2751509 \n\n\nlinear regression shown as below:\n\nlibrary(broom)\n\nWarning: package 'broom' was built under R version 4.3.3\n\nlibrary(knitr)\n\nWarning: package 'knitr' was built under R version 4.3.3\n\nlibrary(dplyr)\n\nmodels &lt;- list(\"Months since last donation\" = lm(mrm2 ~ treatment, data = data),\n  \"Highest previous amount\" = lm(hpa ~ treatment, data = data),\n  \"Female indicator\" = lm(female ~ treatment, data = data))\n\nmodel_output &lt;- lapply(names(models), function(name) {\n  broom::tidy(models[[name]]) %&gt;%mutate(p.value = format.pval(p.value, digits = 5)) %&gt;%\n    mutate(model = name)}) %&gt;%bind_rows()\n\nmodel_output %&gt;%\n  select(model, term, estimate, std.error, statistic, p.value) %&gt;%\n  knitr::kable(digits = 3, caption = \"balance check regressions\")\n\n\nbalance check regressions\n\n\n\n\n\n\n\n\n\n\nmodel\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\nMonths since last donation\n(Intercept)\n12.998\n0.094\n138.979\n&lt; 2e-16\n\n\nMonths since last donation\ntreatment\n0.014\n0.115\n0.119\n0.90489\n\n\nHighest previous amount\n(Intercept)\n58.960\n0.551\n107.005\n&lt;2e-16\n\n\nHighest previous amount\ntreatment\n0.637\n0.675\n0.944\n0.3451\n\n\nFemale indicator\n(Intercept)\n0.283\n0.004\n80.688\n&lt; 2e-16\n\n\nFemale indicator\ntreatment\n-0.008\n0.004\n-1.758\n0.078691"
  },
  {
    "objectID": "blog/MGT495_HW1_.html#experimental-results",
    "href": "blog/MGT495_HW1_.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\nThe interpretation of the results:\nThrough running the t-test, I find the following results:\n•   mrm2: mean (control): 12.998, mean (treatment): 13.012, t = -0.120, p = 0.905; It indicates not statistically significant.\n•   hpa: mean (control): 58.96, mean (treatment): 59.60, t = -0.970, p = 0.332; It indicates not statistically significant.\n•   female: mean (control): 28.3%, mean (treatment): 27.5%, t = 1.754, p = 0.080; It indicates not significant.\nThe results I obtain from t-test validate that the random assignment created statistically comparable groups; all of the p-value is above the 0.05 threshold, which indicates we can not reject the null hypothesis.\nThe results connect to the Table 1 in the paper (Karlan and List, 2007) and show the randomization works as intended and could use to explain the outcome differences in later parts due to the treatments not the imbalance in baseline.\nI use the linear regression part to validate my result from t-test. I run the linear regression model on the same variables and find the following results:\n• mrm2:estimated difference: 0.0137 months, p = 0.905; indicates no significant difference.\n•   hpa: estimated difference: 0.6371 dollars, p = 0.345; indicates no significant difference.\n•   female: estimated difference: -0.0075 (0.75 percentage points fewer women in treatment), p = 0.079; indicates no significant.\nThe results I obtain from linear regression model is similar to the ones I obtain from t-test; none of them are significant difference at 0.05 level.\nBoth t-test and linear regression models yield similar results, which confirm that the group are well-balanced before the experiment started. It supports the internal validity of the design of experiment as shown in the table 1 in Karlan and List(2007) paper. It indicates any outcome downstream differences can be explained to the treatment assignment.\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation. This is like the table 2 panel A shown in the paper.\n\n# use ggplot\nlibrary(ggplot2)\nlibrary(scales)\n\n# group means\ndonate_rate &lt;- data %&gt;% group_by(treatment) %&gt;% summarise(proportion = mean(gave, na.rm = TRUE))\n\n# convert treatment to factor to label\ndonate_rate$treatment &lt;- factor(donate_rate$treatment, labels = c(\"control\", \"treatment\"))\ndonate_rate$label &lt;- paste0(round(donate_rate$proportion * 100, 1), \"%\")\n\n# visualize using barplot\nggplot(donate_rate, aes(x = treatment, y = proportion, fill = treatment)) +\n  geom_bar(stat = \"identity\", width = 0.6) +geom_text(aes(label = label), vjust = -0.5, size = 5)+\n  labs(title = \"donation rate by treatment group\",x = \"group\",y =\"proportion donated\") +\n  scale_y_continuous(labels = percent_format(accuracy = 0.1), limits = c(0, max(donate_rate$proportion) + 0.01))+theme_minimal()\n\n\n\n\n\n\n\n\nNow, it is important to identify if the individuals in treatment group are more likely to make donations than the ones in control group, now run a t-test and linear regression on the binary outcomes.\n\n# t-test\nt.test(gave ~ treatment, data = data)\n\n\n    Welch Two Sample t-test\n\ndata:  gave by treatment\nt = -3.2095, df = 36577, p-value = 0.001331\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -0.006733310 -0.001627399\nsample estimates:\nmean in group 0 mean in group 1 \n     0.01785821      0.02203857 \n\n\n\n#linear regression\nmodel_gave &lt;- lm(gave ~ treatment, data = data)\nmodel_gave_output &lt;- tidy(model_gave)\nmodel_gave_output %&gt;%\n  select(term, estimate, std.error, statistic, p.value) %&gt;%\n  knitr::kable(digits =4, caption = \"Regression:treatment effect on donation rate\")\n\n\nRegression:treatment effect on donation rate\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n0.0179\n0.0011\n16.2246\n0.0000\n\n\ntreatment\n0.0042\n0.0013\n3.1014\n0.0019\n\n\n\n\n\nThe results I get from two sample t-test is:\n•   mean in control group: 1.79%\n•   mean in treatment group: 2.20%\n•   difference in means: 0.42 percentage points\n•   p-value: 0.0013\nSince p-value is 0.0013, which indicates the difference is statistically significant at the 1% level.\nThe results I get from linear regression is: • control group mean: 0.0179 • treatment coefficient: 0.00418 • p-value: 0.0019\nThe estimate above of linear regression confirms the t-test result, which indicates individuals in the treatment group are 0.42% more likely to donate.\nOverall, both results show that receiving a matching grant offer could improve the giving probability, which confirms the finding in table 2 panel A in the paper. It could support the idea that people could respond to perceived impact.\nTo further prove the finding, I run probit regression. The independent variable here is treatment and depend variable is whether the person donate, which replicates the result in table 2 column 1 from original paper.\n\n# probit model:\nprobit_model &lt;- glm(gave ~ treatment, data = data, family = binomial(link = \"probit\"))\nprobit_output &lt;- tidy(probit_model)\n\nprobit_output %&gt;%\n  select(term, estimate, std.error, statistic, p.value) %&gt;%\n  knitr::kable(digits = 4, caption = \"Probit model\")\n\n\nProbit model\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n-2.1001\n0.0233\n-90.0739\n0.0000\n\n\ntreatment\n0.0868\n0.0279\n3.1130\n0.0019\n\n\n\n\n\nThe output I get from probit model:\n•   intercept is -2.100\n•   treatment coefficient is 0.087\n•   p-value is 0.0019\nThe treatment coefficient is 0.087, which indicates receiving a matching offer could increase the giving probability. It also confirms the previous conclusion.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate. I constrain the sample to treatment group and use pairwise t-test:\n\n#t-test between 2:1 and 1:1\ntreatment_only &lt;- filter(data, treatment == 1) #treatment group subset\ngave_ratio1 &lt;- filter(treatment_only, ratio == 1)$gave #ratio group\ngave_ratio2 &lt;- filter(treatment_only, ratio == 2)$gave\ngave_ratio3 &lt;- filter(treatment_only, ratio == 3)$gave\nt.test(gave_ratio2, gave_ratio1)\n\n\n    Welch Two Sample t-test\n\ndata:  gave_ratio2 and gave_ratio1\nt = 0.96505, df = 22225, p-value = 0.3345\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.001942773  0.005711275\nsample estimates:\n mean of x  mean of y \n0.02263338 0.02074912 \n\n\n\n#t-test between 3:1 and 1:1\nt.test(gave_ratio3, gave_ratio1)\n\n\n    Welch Two Sample t-test\n\ndata:  gave_ratio3 and gave_ratio1\nt = 1.015, df = 22215, p-value = 0.3101\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.001847501  0.005816051\nsample estimates:\n mean of x  mean of y \n0.02273340 0.02074912 \n\n\n\n#t-test between 3:1 and 2:1\nt.test(gave_ratio3, gave_ratio2)\n\n\n    Welch Two Sample t-test\n\ndata:  gave_ratio3 and gave_ratio2\nt = 0.050116, df = 22261, p-value = 0.96\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.003811996  0.004012044\nsample estimates:\n mean of x  mean of y \n0.02273340 0.02263338 \n\n\nFor the first comparison, which is 2:1 to 1:1 case, the mean response rate o9f 2:1 is about 2.26% and mean response rate of 1:1 is about 2.07%, p-value in this case is 0.335, which is not significant at level 0.05; for the second case, which is 3:1 to 1:1 match case, the mean response rate for 3:1 is 2.27% and for 1:1 is 2.07%, and p-value is 0.310, which indicates not significant at level 0.05; for the third case, mean response rate for 3:1 is 2.27% and for 2:1 is 2:26%. Across all three comparison, the donation rate is pretty small and not significant. This support the page 8 of the paper where the author mentions “….figures suggest that increasing the match ratio from 1:1 to 2:1 or 3:1 does not increase the likelihood of giving.” (Karlan and List, 2007). It indicates that announcing match does not play a role in affecting the donation rate and increasing match ratio beyond 1:1 seems not contribute to boost donation.The match matters more than the size.\nNow, I assess the same issue using a regression. I create dummy variables for each match ratio within the treatment group (ratio1, ratio2, and ratio3). If include all the three groups, I will have a multicollinearity, soo I exclude ratio 1 and make it as a reference group, which allows ratio2 and ratio3 to be explained as differences to 1:1 match.\n\n# regression\nmodel_dummies &lt;- lm(gave ~ ratio2 + ratio3, data = treatment_only)\nmodel_dummies_output &lt;- tidy(model_dummies)\n\n# clean table\nmodel_dummies_output %&gt;%\n  select(term, estimate, std.error, statistic, p.value) %&gt;%\n  knitr::kable(digits = 4, caption = \"Regression: differences across match ratios\")\n\n\nRegression: differences across match ratios\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n0.0207\n0.0014\n14.9122\n0.0000\n\n\nratio2\n0.0019\n0.0020\n0.9576\n0.3383\n\n\nratio3\n0.0020\n0.0020\n1.0083\n0.3133\n\n\n\n\n\nThe results of the regression shows the intercept 0.02075 represents the mean donation for 1:1 match group; coefficient for ratio2 is about 0.00188 and has 0.19% increase. It is not significant since p=0.338 which is above 5% level. The coefficient for ratio3 is abou 0.00198 and is still not significant at the 5% level (p-value is 0.313).\nThe results I get here are consistent with the early calculation using t-test, which confirm the paper’s perspective on 8 page.\nTo calculate the response rate difference between the 1:1 and 2:1 match ratios and 2:1 and 3:1 ratios, I need to use raw group means and regression coefficients to compare:\n\n# find the group mean response rates\ntreatment_only &lt;- filter(data, treatment == 1)\ngroup_rates &lt;- treatment_only %&gt;%\n  group_by(ratio) %&gt;%\n  summarise(rate = mean(gave))\n\ngroup_rates\n\n# A tibble: 3 × 2\n  ratio       rate\n  &lt;dbl+lbl&gt;  &lt;dbl&gt;\n1 1         0.0207\n2 2         0.0226\n3 3         0.0227\n\n\n\n#for regression coefficients:\nmodel_dummies &lt;- lm(gave ~ ratio2 + ratio3, data = treatment_only)\n\nmodel_amount_output &lt;- tidy(model_dummies)\n\nmodel_amount_output %&gt;%\n  select(term, estimate, std.error, statistic, p.value) %&gt;%\n  knitr::kable(digits = 4, caption = \"Regression: treatment effect on donation amount\")\n\n\nRegression: treatment effect on donation amount\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n0.0207\n0.0014\n14.9122\n0.0000\n\n\nratio2\n0.0019\n0.0020\n0.9576\n0.3383\n\n\nratio3\n0.0020\n0.0020\n1.0083\n0.3133\n\n\n\n\n\nFrom the results I obtained above, both methods shows that from 1:1 to 2:1, it has a small increase in giving; 2:1 to 3:1 shows no additional benefits with only 0.01% increased. This further confirmed the argument in the paper on page 8 where the author mentions \"figures suggest that increasing the match ratio from 1:1 to 2:1 or 3:1 does not increase the likelihood of giving\".\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\nTo further see if the macth donations would influence the size of charitable contribution, I use a t-test to compare between the treatment and control group:\n\n#choose do a t-test\nt.test(amount ~ treatment, data = data)\n\n\n    Welch Two Sample t-test\n\ndata:  amount by treatment\nt = -1.9183, df = 36216, p-value = 0.05509\nalternative hypothesis: true difference in means between group 0 and group 1 is not equal to 0\n95 percent confidence interval:\n -0.310555423  0.003344493\nsample estimates:\nmean in group 0 mean in group 1 \n      0.8132678       0.9668733 \n\n\nIt shows that the average for control group is 0.813 and for treatment group is 0.967, difference is about 0.154 and p-value is about 0.055, which is not significant at 5% level. It is very close to 5% and could suggest there is a positive relationship. This result further reinforces that match offers increase participation.\nNow, limit the data to just people who made a donation and repeat the previous analysis. This regression allows us to analyze how much respondents donate conditional on donating some positive amount.\n\n#only keep data with donation\ndonated_only &lt;- filter(data, amount &gt; 0)\n\n# regression analysis\nmodel_donation_only &lt;- lm(amount ~ treatment, data = donated_only)\nmodel_amount_output &lt;- tidy(model_donation_only)\nmodel_amount_output %&gt;%\n  select(term, estimate, std.error, statistic, p.value) %&gt;%\n  knitr::kable(digits = 4, caption = \"Regression: treatment effect on donation amount\")\n\n\nRegression: treatment effect on donation amount\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n45.5403\n2.4234\n18.7921\n0.0000\n\n\ntreatment\n-1.6684\n2.8724\n-0.5808\n0.5615\n\n\n\n\n\nAfter I dive into the people who donate, the result shows that the treatment group has 1.67 dollar less on average. The p value here is 0.5615, which larger than 5% level and it is not significant. It indicates match offer encourages people to donate but it does not change the amount of values they give. I cannot interpret it causally since it only contains the donors.\nThen I make plots: one for the treatment group and one for the control. Each plot is a histogram of the donation amounts only among people who donated. I add a red vertical bar to indicate the sample average for each plot.\n\nlibrary(ggplot2)\nlibrary(dplyr)\n\ndonated &lt;- data %&gt;% filter(gave == 1) #only keep people who make donation\n\n# 0 = control, 1 = treatment\ndonated &lt;- donated %&gt;% mutate(treatment = factor(treatment, levels = c(0, 1), labels = c(\"control\", \"treatment\")))\n#avg donation\navg_amounts &lt;- donated %&gt;%group_by(treatment) %&gt;%summarise(mean_amount = mean(amount, na.rm = TRUE))\n\n#visualize uisng histogram\nggplot(donated, aes(x = amount)) +\n  geom_histogram(binwidth = 4,color = \"blue\") +facet_wrap(~treatment, scales = \"free_y\") +\n  geom_vline(data = avg_amounts, aes(xintercept = mean_amount), color = \"red\", linetype = \"dashed\") +\n  labs( x = \"donation ($)\", y = \"count\") +theme_minimal()"
  },
  {
    "objectID": "blog/MGT495_HW1_.html#simulation-experiment",
    "href": "blog/MGT495_HW1_.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\n\nset.seed(1)\n\n#simulate 100,000 draws for control with p = 0.018 and for treatment with p = 0.022\ncontrols &lt;- rbinom(100000, 1, 0.018)\ntreatments &lt;- rbinom(100000, 1, 0.022)\n\n#differences\ndif&lt;- treatments - controls\n#cumulative average\ncum_avg &lt;- cumsum(dif) / seq_along(dif)\n\n#visualize\nggplot(data.frame(n = 1:100000, cum_avg = cum_avg), aes(x = n, y = cum_avg)) +\n  geom_line(color = \"blue\") +geom_hline(yintercept = 0.004, color = \"red\") +\n  labs(title = \"cumulative average using LLN\",x = \"simulations\",\n    y =\"cumulative average of differences\") +theme_minimal()\n\n\n\n\n\n\n\n\nFrom the graph above, I could see how the cumulative average of differences changes with the increasing in simulations. As simulations increases, the cumulative average of differences converge to 0.004, which shows me the Law of Large Numbers. This reflects a very useful thing that demonstrates the empirical estimates made not due to random noise and could approach the true means.\n\n\nCentral Limit Theorem\nThen, I make 4 histograms like those on slide 44 from our first class at sample sizes 50, 200, 500, and 1000.\nTo do this for a sample size of e.g. 50, take 50 draws from each of the control and treatment distributions, and calculate the average difference between those draws. Then repeat that process 999 more times so that you have 1000 averages. Then I plot the histogram of those averages.\n\nset.seed(1)\nlibrary(tibble)\n\n# function\nsimulate_diff &lt;- function(n, reps = 1000, p1= 0.018, p2 = 0.022) {\n  replicate(reps, {control &lt;- rbinom(n, 1, p1)\n  treatment &lt;- rbinom(n, 1, p2)\n  mean(treatment) - mean(control)})}\n\ndiff_50 &lt;- simulate_diff(50) #sample size\ndiff_200 &lt;- simulate_diff(200)\ndiff_500 &lt;- simulate_diff(500)\ndiff_1000 &lt;- simulate_diff(1000)\n\ndf &lt;- tibble(diff = c(diff_50, diff_200, diff_500, diff_1000),\n  n = factor(rep(c(50, 200, 500, 1000), each = 1000),levels = c(50, 200, 500, 1000)))\n#plot\nggplot(df, aes(x = diff)) +geom_histogram(binwidth = 0.01, fill = \"blue\") +\n  facet_wrap(~ n, scales = \"free_y\") +geom_vline(xintercept = 0, color=\"red\") +\n  labs(title = \"sampling distribution of average differences\",\n    x = \"average difference (treatment - control)\", y = \"count\") +theme_minimal()"
  },
  {
    "objectID": "blog/project2/index.html",
    "href": "blog/project2/index.html",
    "title": "This is Project 2",
    "section": "",
    "text": "I cleaned some data\n\n\n\nI analyzed the data\n#| message: false\nlibrary(tidyverse) mtcars |&gt; ggplot(aes(x = wt, y = mpg)) + geom_point()"
  },
  {
    "objectID": "blog/project2/index.html#section-1-data",
    "href": "blog/project2/index.html#section-1-data",
    "title": "This is Project 2",
    "section": "",
    "text": "I cleaned some data"
  },
  {
    "objectID": "blog/project2/index.html#section-2-analysis",
    "href": "blog/project2/index.html#section-2-analysis",
    "title": "This is Project 2",
    "section": "",
    "text": "I analyzed the data\n#| message: false\nlibrary(tidyverse) mtcars |&gt; ggplot(aes(x = wt, y = mpg)) + geom_point()"
  }
]