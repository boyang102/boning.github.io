[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "My Projects",
    "section": "",
    "text": "This is Project 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis is Project 2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n\n\n\n\nBoning Yang\n\n\nMay 1, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/hw2.html",
    "href": "blog/hw2.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\nLoad the data for this project:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nblueprinty &lt;- read_csv(\"blueprinty.csv\")\n\nRows: 1500 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): region\ndbl (3): patents, age, iscustomer\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(blueprinty)\n\n# A tibble: 6 × 4\n  patents region      age iscustomer\n    &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1       0 Midwest    32.5          0\n2       3 Southwest  37.5          0\n3       4 Northwest  27            1\n4       3 Northeast  24.5          0\n5       3 Southwest  37            0\n6       6 Northeast  29.5          1\n\n\nNow, compare histograms and means of number of patents by customer status.\n\n#means of number of patents by customer status\nblueprinty %&gt;%group_by(iscustomer) %&gt;%\n  summarise(mean_patents = mean(patents),sd_patents = sd(patents),n = n())\n\n# A tibble: 2 × 4\n  iscustomer mean_patents sd_patents     n\n       &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt; &lt;int&gt;\n1          0         3.47       2.23  1019\n2          1         4.13       2.55   481\n\n#histogram\nggplot(blueprinty, aes(x = patents, fill = factor(iscustomer))) +\n  geom_histogram(binwidth = 1, position = \"dodge\") +\n  scale_fill_manual(values = c(\"gray\", \"pink\"),labels = c(\"Non-Customer\", \"Customer\")) +\n  labs(title = \"Histogram of Patents by Customer Status\",\n    x = \"Number of Patents\",y = \"Count\",fill = \"Customer Status\") +theme_minimal()\n\n\n\n\n\n\n\n\nFrom above analysis, I observe that on average the Blueprinty customers have ~4.13 patents and for the non-customers, they have ~3.47 patents. By comparing this, we know that for the Blueprinty customers, they are more likely to have higher patent counts than the non-customers.The histogram also supports this hypothesis. I will use Poisson regression to better estimate it further.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n#compare avg ages\nblueprinty %&gt;%group_by(iscustomer) %&gt;%summarise(mean_age=mean(age),sd_age = sd(age))\n\n# A tibble: 2 × 3\n  iscustomer mean_age sd_age\n       &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n1          0     26.1   6.95\n2          1     26.9   7.81\n\n# age distribution\nggplot(blueprinty, aes(x = factor(iscustomer), y =age, fill= factor(iscustomer))) +\n  geom_boxplot() +scale_fill_manual(values = c(\"gray\", \"pink\"),labels = c(\"non customers\", \"customer\")) +labs(title = \"age vs customer status\",\n    x = \"status of customers\",y = \"age\",fill = \"customer status\")+theme_minimal()\n\n\n\n\n\n\n\n#regional location\nblueprinty %&gt;%count(region, iscustomer) %&gt;%group_by(region) %&gt;%\n  mutate(percent = n/sum(n)) %&gt;%ggplot(aes(x = region, y=percent, fill=factor(iscustomer))) +\n  geom_bar(stat = \"identity\") +scale_fill_manual(values = c(\"gray\", \"lightblue\"),\n                    labels = c(\"non customer\", \"customer\")) +\n  labs(title = \"regional distribution vs customer status\",x = \"regional location\",\n    y = \"proportion\",fill = \"status of customers\") +theme_minimal()\n\n\n\n\n\n\n\n\nCompare regions and ages by customer status, I notice that the mean age for non-customers is about 26.1 while the mean age for customers is about 26.9, which is slightly older here. The box plot shows the distribution of ages for the two customer status; for the customers (pink box), it has higher median and wider spread.\nFor the regional location part, the distribution here seems varies with customers’ status. There is a higher portion of customers located in Northeast than other areas, which indicates the customer adoption is not uniform and may not be random. Overall, both of them shows the importance of controlling age and region in the analysis to minimize the bias in the data.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nWrite down the mathematically the likelihood for_ \\(Y \\sim \\text{Poisson}(\\lambda)\\). Note that \\(f(Y|\\lambda) = e^{-\\lambda}\\lambda^Y/Y!\\).\n–For a single observation Y_i ~ Poisson(lambda_i), the probability mass function is: f(Y_i | lambda_i)=exp(-lambda_i)*lambda_i^Y_i/factorial(Y_i) –For multiple independent observations Y_1, …,Y_n with corresponding lambda_1, …, lambda_n, the joint likelihood is the product of individual likelihoods: L(lambda)=Π[ exp(-lambda_i)* lambda_i^Y_i/Y_i!] over i=1 to n –Taking the natural log of the likelihood gives the log-likelihood: logL(lambda)=Σ[-lambda_i+ Y_i*log(lambda_i)-log(Y_i!)] over i = 1 to n –log-likelihood function is to take lambda and observed Y\nCode the likelihood (or log-likelihood) function for the Poisson model. This is a function of lambda and Y. For example:\npoisson_loglikelihood &lt;- function(lambda, Y){\n   sum(-lambda+Y*log(lambda)-lgamma(Y+1))}\nUse the function to plot lambda on the horizontal axis and the likelihood (or log-likelihood) on the vertical axis for a range of lambdas (use the observed number of patents as the input for Y).\n\npoisson_loglikelihood &lt;- function(lambda, Y) {sum(-lambda+Y*log(lambda) - lgamma(Y+1))}\n\n#use one observed Y value\nY_eg &lt;- blueprinty$patents[10]\n#lambda values sequence\nlambs &lt;- seq(0.1, 10, by = 0.1)\n#find log-likelihood for each lambda\nloglik_vals &lt;- sapply(lambs, function(l) poisson_loglikelihood(l,Y_eg))\n\n# plot log-likelihood curve\nplot(lambs, loglik_vals, type = \"l\",\n     col = \"blue\", xlab = expression(lambda),\n     ylab = \"Log Likelihood\",\n     main = paste(\"Log Likelihood at Y =\", Y_eg))\nabline(v=Y_eg, col= \"pink\")\n\n\n\n\n\n\n\n\nLet’s ttake the first derivative of log-likelihood, set it equal to zero and solve for lambda. I find lambda_mle is Ybar, which “feels right” because the mean of a Poisson distribution is lambda.\nSo here, I begin with the log-likelihood function for n i.i.d. observations from a Poisson distribution with the common mean , I get:\nL() = _{i=1}^n ( -+ Y_i - Y_i! )\nI then drop the term Y_i! because it does not depend on .Now, it simplify to the following equation:\nL() = -n+ _{i=1}^n Y_i\nTaking the first derivative with respect to :\n-n + _{i=1}^n Y_i\nNow, let the derivative to be zero and I want to use it to find the maximum value.Then I find:\n= _{i=1}^n Y_i = {Y}\nNow, find the MLE of lambada by maximizing the log-likelihood function using R’s optim().\n\n#negated log-likelihood function for minimization\nneg_ll &lt;- function(lambda, Y) {\n#avoid log(0)\n  if (lambda &lt;= 0) return(Inf)\n  return(-sum(-lambda + Y * log(lambda) - lgamma(Y + 1)))}\nY_data &lt;- blueprinty$patents\n#minimize the negative log-likelihood\nmle_result &lt;- optim(par = 1, fn = neg_ll,Y=Y_data,method=\"BFGS\",hessian = TRUE)\nlambda_mle &lt;- mle_result$par #estimate lambda\nlambda_mle\n\n[1] 3.68467\n\nmean(Y_data)\n\n[1] 3.684667\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n# update log-likelihood for Poisson\npoisson_regression_likelihood &lt;- function(beta, Y, X) {\n  # X:covariates matrix, beta:vector of coefficients\n  lambda &lt;- exp(X %*% beta) #inverse\n  loglik &lt;- sum(-lambda+Y*log(lambda)-lgamma(Y+1))\n  return(-loglik)}\n\n\nblueprinty &lt;- blueprinty %&gt;%mutate(age2=age^2)\nX &lt;- model.matrix(~age + age2+region+iscustomer, data=blueprinty)\nY &lt;- blueprinty$patents\n\n#let initial parameter as 0\nbeta0 &lt;- rep(0, ncol(X))\nresult &lt;- optim(par=beta0,fn = poisson_regression_likelihood,Y=Y,X=X,\n                method = \"BFGS\",hessian = TRUE)\n\nbeta_hat &lt;- result$par\n#calculate the standard errors by inverse hessian\nhessian_mat &lt;- result$hessian\nse_beta &lt;- sqrt(diag(solve(hessian_mat)))\n\ncoef_table &lt;- data.frame(Term = colnames(X),Estimate = beta_hat,Std_Error = se_beta)\nknitr::kable(coef_table, digits = 4,\n             caption = \"poisson coefficient estimates&standard errors\")\n\n\npoisson coefficient estimates&standard errors\n\n\nTerm\nEstimate\nStd_Error\n\n\n\n\n(Intercept)\n-0.1257\n0.1122\n\n\nage\n0.1158\n0.0064\n\n\nage2\n-0.0022\n0.0001\n\n\nregionNortheast\n-0.0246\n0.0434\n\n\nregionNorthwest\n-0.0348\n0.0529\n\n\nregionSouth\n-0.0054\n0.0524\n\n\nregionSouthwest\n-0.0378\n0.0472\n\n\niscustomer\n0.0607\n0.0321\n\n\n\n\n\nFrom the above table, the age coefficient is positive since it says 0.1158, while the age square coefficient is negative at –0.0022. This suggests a concave relationship between the firm age and patent count and it means the older firms tend to produce more patents up to a point, after which the effect disappear. The iscustomer coefficient is positive at 0.0607. It indicates that firms using Blueprinty’s software may have a higher expected number of patents. In addition, the regional effects are all negative compared to the omitted base region. However, none of these appear statistically significant at conventional levels.\nCheck my results using R’s glm() function:\n\nglm_model &lt;- glm(patents ~ age+I(age^2)+region+iscustomer,data=blueprinty,family=poisson())\nsummary(glm_model)\n\n\nCall:\nglm(formula = patents ~ age + I(age^2) + region + iscustomer, \n    family = poisson(), data = blueprinty)\n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)     -0.508920   0.183179  -2.778  0.00546 ** \nage              0.148619   0.013869  10.716  &lt; 2e-16 ***\nI(age^2)        -0.002971   0.000258 -11.513  &lt; 2e-16 ***\nregionNortheast  0.029170   0.043625   0.669  0.50372    \nregionNorthwest -0.017574   0.053781  -0.327  0.74383    \nregionSouth      0.056561   0.052662   1.074  0.28281    \nregionSouthwest  0.050576   0.047198   1.072  0.28391    \niscustomer       0.207591   0.030895   6.719 1.83e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 2362.5  on 1499  degrees of freedom\nResidual deviance: 2143.3  on 1492  degrees of freedom\nAIC: 6532.1\n\nNumber of Fisher Scoring iterations: 5\n\n\n\n#combine glm and optim output all together in a table\ncompare &lt;- data.frame(\n  Term = names(coef(glm_model)),\n  Estimate_glm = coef(glm_model),\n  Estimate_optim = coef_table$Estimate,\n  SE_glm = summary(glm_model)$coefficients[, \"Std. Error\"],\n  SE_optim = coef_table$Std_Error)\n\nknitr::kable(compare, digits=4, caption=\"comparison of glm() and optim() estimates\")\n\n\ncomparison of glm() and optim() estimates\n\n\n\n\n\n\n\n\n\n\n\nTerm\nEstimate_glm\nEstimate_optim\nSE_glm\nSE_optim\n\n\n\n\n(Intercept)\n(Intercept)\n-0.5089\n-0.1257\n0.1832\n0.1122\n\n\nage\nage\n0.1486\n0.1158\n0.0139\n0.0064\n\n\nI(age^2)\nI(age^2)\n-0.0030\n-0.0022\n0.0003\n0.0001\n\n\nregionNortheast\nregionNortheast\n0.0292\n-0.0246\n0.0436\n0.0434\n\n\nregionNorthwest\nregionNorthwest\n-0.0176\n-0.0348\n0.0538\n0.0529\n\n\nregionSouth\nregionSouth\n0.0566\n-0.0054\n0.0527\n0.0524\n\n\nregionSouthwest\nregionSouthwest\n0.0506\n-0.0378\n0.0472\n0.0472\n\n\niscustomer\niscustomer\n0.2076\n0.0607\n0.0309\n0.0321\n\n\n\n\n\nFrom the above results, it verifies the estimates from glm() and optim() are mostly consistent,and this further confirms the right of the MLE.\nThe table demonstrates that both age and age square show a clear inverted-U relationship with the patent counts. The iscustomer has a positive effect which suggests Blueprinty users file more patents, although the size differs slightly between each method. Third, the region effects are small and not statistically significant at the level, while the intercept varies more, mostly may due to optimizer sensitivity. Overall, the firm age and customer status are the most influential predictors.\n\n# counterfactual covariate matrices\nX_0 &lt;- X\nX_0[, \"iscustomer\"] &lt;- 0\nX_1 &lt;- X\nX_1[, \"iscustomer\"] &lt;- 1\n#predict lambda_i for both scenarios\nlambda_0 &lt;- exp(X_0 %*% beta_hat)\nlambda_1 &lt;- exp(X_1 %*% beta_hat)\ndiff &lt;- lambda_1 - lambda_0 #average treatment effect\nate &lt;- mean(diff)\nate\n\n[1] 0.2178843\n\n\nBased on above results, the firms that use Blueprinty’s software are predicted to receive ~0.22 more patents on average than similar firms which do not use the software. This estimated effect is small, but it can support the claim that Blueprinty may help improve the patenting outcomes."
  },
  {
    "objectID": "blog/hw2.html#blueprinty-case-study",
    "href": "blog/hw2.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\nLoad the data for this project:\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nblueprinty &lt;- read_csv(\"blueprinty.csv\")\n\nRows: 1500 Columns: 4\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): region\ndbl (3): patents, age, iscustomer\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nhead(blueprinty)\n\n# A tibble: 6 × 4\n  patents region      age iscustomer\n    &lt;dbl&gt; &lt;chr&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1       0 Midwest    32.5          0\n2       3 Southwest  37.5          0\n3       4 Northwest  27            1\n4       3 Northeast  24.5          0\n5       3 Southwest  37            0\n6       6 Northeast  29.5          1\n\n\nNow, compare histograms and means of number of patents by customer status.\n\n#means of number of patents by customer status\nblueprinty %&gt;%group_by(iscustomer) %&gt;%\n  summarise(mean_patents = mean(patents),sd_patents = sd(patents),n = n())\n\n# A tibble: 2 × 4\n  iscustomer mean_patents sd_patents     n\n       &lt;dbl&gt;        &lt;dbl&gt;      &lt;dbl&gt; &lt;int&gt;\n1          0         3.47       2.23  1019\n2          1         4.13       2.55   481\n\n#histogram\nggplot(blueprinty, aes(x = patents, fill = factor(iscustomer))) +\n  geom_histogram(binwidth = 1, position = \"dodge\") +\n  scale_fill_manual(values = c(\"gray\", \"pink\"),labels = c(\"Non-Customer\", \"Customer\")) +\n  labs(title = \"Histogram of Patents by Customer Status\",\n    x = \"Number of Patents\",y = \"Count\",fill = \"Customer Status\") +theme_minimal()\n\n\n\n\n\n\n\n\nFrom above analysis, I observe that on average the Blueprinty customers have ~4.13 patents and for the non-customers, they have ~3.47 patents. By comparing this, we know that for the Blueprinty customers, they are more likely to have higher patent counts than the non-customers.The histogram also supports this hypothesis. I will use Poisson regression to better estimate it further.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\n\n#compare avg ages\nblueprinty %&gt;%group_by(iscustomer) %&gt;%summarise(mean_age=mean(age),sd_age = sd(age))\n\n# A tibble: 2 × 3\n  iscustomer mean_age sd_age\n       &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n1          0     26.1   6.95\n2          1     26.9   7.81\n\n# age distribution\nggplot(blueprinty, aes(x = factor(iscustomer), y =age, fill= factor(iscustomer))) +\n  geom_boxplot() +scale_fill_manual(values = c(\"gray\", \"pink\"),labels = c(\"non customers\", \"customer\")) +labs(title = \"age vs customer status\",\n    x = \"status of customers\",y = \"age\",fill = \"customer status\")+theme_minimal()\n\n\n\n\n\n\n\n#regional location\nblueprinty %&gt;%count(region, iscustomer) %&gt;%group_by(region) %&gt;%\n  mutate(percent = n/sum(n)) %&gt;%ggplot(aes(x = region, y=percent, fill=factor(iscustomer))) +\n  geom_bar(stat = \"identity\") +scale_fill_manual(values = c(\"gray\", \"lightblue\"),\n                    labels = c(\"non customer\", \"customer\")) +\n  labs(title = \"regional distribution vs customer status\",x = \"regional location\",\n    y = \"proportion\",fill = \"status of customers\") +theme_minimal()\n\n\n\n\n\n\n\n\nCompare regions and ages by customer status, I notice that the mean age for non-customers is about 26.1 while the mean age for customers is about 26.9, which is slightly older here. The box plot shows the distribution of ages for the two customer status; for the customers (pink box), it has higher median and wider spread.\nFor the regional location part, the distribution here seems varies with customers’ status. There is a higher portion of customers located in Northeast than other areas, which indicates the customer adoption is not uniform and may not be random. Overall, both of them shows the importance of controlling age and region in the analysis to minimize the bias in the data.\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nWrite down the mathematically the likelihood for_ \\(Y \\sim \\text{Poisson}(\\lambda)\\). Note that \\(f(Y|\\lambda) = e^{-\\lambda}\\lambda^Y/Y!\\).\n–For a single observation Y_i ~ Poisson(lambda_i), the probability mass function is: f(Y_i | lambda_i)=exp(-lambda_i)*lambda_i^Y_i/factorial(Y_i) –For multiple independent observations Y_1, …,Y_n with corresponding lambda_1, …, lambda_n, the joint likelihood is the product of individual likelihoods: L(lambda)=Π[ exp(-lambda_i)* lambda_i^Y_i/Y_i!] over i=1 to n –Taking the natural log of the likelihood gives the log-likelihood: logL(lambda)=Σ[-lambda_i+ Y_i*log(lambda_i)-log(Y_i!)] over i = 1 to n –log-likelihood function is to take lambda and observed Y\nCode the likelihood (or log-likelihood) function for the Poisson model. This is a function of lambda and Y. For example:\npoisson_loglikelihood &lt;- function(lambda, Y){\n   sum(-lambda+Y*log(lambda)-lgamma(Y+1))}\nUse the function to plot lambda on the horizontal axis and the likelihood (or log-likelihood) on the vertical axis for a range of lambdas (use the observed number of patents as the input for Y).\n\npoisson_loglikelihood &lt;- function(lambda, Y) {sum(-lambda+Y*log(lambda) - lgamma(Y+1))}\n\n#use one observed Y value\nY_eg &lt;- blueprinty$patents[10]\n#lambda values sequence\nlambs &lt;- seq(0.1, 10, by = 0.1)\n#find log-likelihood for each lambda\nloglik_vals &lt;- sapply(lambs, function(l) poisson_loglikelihood(l,Y_eg))\n\n# plot log-likelihood curve\nplot(lambs, loglik_vals, type = \"l\",\n     col = \"blue\", xlab = expression(lambda),\n     ylab = \"Log Likelihood\",\n     main = paste(\"Log Likelihood at Y =\", Y_eg))\nabline(v=Y_eg, col= \"pink\")\n\n\n\n\n\n\n\n\nLet’s ttake the first derivative of log-likelihood, set it equal to zero and solve for lambda. I find lambda_mle is Ybar, which “feels right” because the mean of a Poisson distribution is lambda.\nSo here, I begin with the log-likelihood function for n i.i.d. observations from a Poisson distribution with the common mean , I get:\nL() = _{i=1}^n ( -+ Y_i - Y_i! )\nI then drop the term Y_i! because it does not depend on .Now, it simplify to the following equation:\nL() = -n+ _{i=1}^n Y_i\nTaking the first derivative with respect to :\n-n + _{i=1}^n Y_i\nNow, let the derivative to be zero and I want to use it to find the maximum value.Then I find:\n= _{i=1}^n Y_i = {Y}\nNow, find the MLE of lambada by maximizing the log-likelihood function using R’s optim().\n\n#negated log-likelihood function for minimization\nneg_ll &lt;- function(lambda, Y) {\n#avoid log(0)\n  if (lambda &lt;= 0) return(Inf)\n  return(-sum(-lambda + Y * log(lambda) - lgamma(Y + 1)))}\nY_data &lt;- blueprinty$patents\n#minimize the negative log-likelihood\nmle_result &lt;- optim(par = 1, fn = neg_ll,Y=Y_data,method=\"BFGS\",hessian = TRUE)\nlambda_mle &lt;- mle_result$par #estimate lambda\nlambda_mle\n\n[1] 3.68467\n\nmean(Y_data)\n\n[1] 3.684667\n\n\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\n# update log-likelihood for Poisson\npoisson_regression_likelihood &lt;- function(beta, Y, X) {\n  # X:covariates matrix, beta:vector of coefficients\n  lambda &lt;- exp(X %*% beta) #inverse\n  loglik &lt;- sum(-lambda+Y*log(lambda)-lgamma(Y+1))\n  return(-loglik)}\n\n\nblueprinty &lt;- blueprinty %&gt;%mutate(age2=age^2)\nX &lt;- model.matrix(~age + age2+region+iscustomer, data=blueprinty)\nY &lt;- blueprinty$patents\n\n#let initial parameter as 0\nbeta0 &lt;- rep(0, ncol(X))\nresult &lt;- optim(par=beta0,fn = poisson_regression_likelihood,Y=Y,X=X,\n                method = \"BFGS\",hessian = TRUE)\n\nbeta_hat &lt;- result$par\n#calculate the standard errors by inverse hessian\nhessian_mat &lt;- result$hessian\nse_beta &lt;- sqrt(diag(solve(hessian_mat)))\n\ncoef_table &lt;- data.frame(Term = colnames(X),Estimate = beta_hat,Std_Error = se_beta)\nknitr::kable(coef_table, digits = 4,\n             caption = \"poisson coefficient estimates&standard errors\")\n\n\npoisson coefficient estimates&standard errors\n\n\nTerm\nEstimate\nStd_Error\n\n\n\n\n(Intercept)\n-0.1257\n0.1122\n\n\nage\n0.1158\n0.0064\n\n\nage2\n-0.0022\n0.0001\n\n\nregionNortheast\n-0.0246\n0.0434\n\n\nregionNorthwest\n-0.0348\n0.0529\n\n\nregionSouth\n-0.0054\n0.0524\n\n\nregionSouthwest\n-0.0378\n0.0472\n\n\niscustomer\n0.0607\n0.0321\n\n\n\n\n\nFrom the above table, the age coefficient is positive since it says 0.1158, while the age square coefficient is negative at –0.0022. This suggests a concave relationship between the firm age and patent count and it means the older firms tend to produce more patents up to a point, after which the effect disappear. The iscustomer coefficient is positive at 0.0607. It indicates that firms using Blueprinty’s software may have a higher expected number of patents. In addition, the regional effects are all negative compared to the omitted base region. However, none of these appear statistically significant at conventional levels.\nCheck my results using R’s glm() function:\n\nglm_model &lt;- glm(patents ~ age+I(age^2)+region+iscustomer,data=blueprinty,family=poisson())\nsummary(glm_model)\n\n\nCall:\nglm(formula = patents ~ age + I(age^2) + region + iscustomer, \n    family = poisson(), data = blueprinty)\n\nCoefficients:\n                 Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)     -0.508920   0.183179  -2.778  0.00546 ** \nage              0.148619   0.013869  10.716  &lt; 2e-16 ***\nI(age^2)        -0.002971   0.000258 -11.513  &lt; 2e-16 ***\nregionNortheast  0.029170   0.043625   0.669  0.50372    \nregionNorthwest -0.017574   0.053781  -0.327  0.74383    \nregionSouth      0.056561   0.052662   1.074  0.28281    \nregionSouthwest  0.050576   0.047198   1.072  0.28391    \niscustomer       0.207591   0.030895   6.719 1.83e-11 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 2362.5  on 1499  degrees of freedom\nResidual deviance: 2143.3  on 1492  degrees of freedom\nAIC: 6532.1\n\nNumber of Fisher Scoring iterations: 5\n\n\n\n#combine glm and optim output all together in a table\ncompare &lt;- data.frame(\n  Term = names(coef(glm_model)),\n  Estimate_glm = coef(glm_model),\n  Estimate_optim = coef_table$Estimate,\n  SE_glm = summary(glm_model)$coefficients[, \"Std. Error\"],\n  SE_optim = coef_table$Std_Error)\n\nknitr::kable(compare, digits=4, caption=\"comparison of glm() and optim() estimates\")\n\n\ncomparison of glm() and optim() estimates\n\n\n\n\n\n\n\n\n\n\n\nTerm\nEstimate_glm\nEstimate_optim\nSE_glm\nSE_optim\n\n\n\n\n(Intercept)\n(Intercept)\n-0.5089\n-0.1257\n0.1832\n0.1122\n\n\nage\nage\n0.1486\n0.1158\n0.0139\n0.0064\n\n\nI(age^2)\nI(age^2)\n-0.0030\n-0.0022\n0.0003\n0.0001\n\n\nregionNortheast\nregionNortheast\n0.0292\n-0.0246\n0.0436\n0.0434\n\n\nregionNorthwest\nregionNorthwest\n-0.0176\n-0.0348\n0.0538\n0.0529\n\n\nregionSouth\nregionSouth\n0.0566\n-0.0054\n0.0527\n0.0524\n\n\nregionSouthwest\nregionSouthwest\n0.0506\n-0.0378\n0.0472\n0.0472\n\n\niscustomer\niscustomer\n0.2076\n0.0607\n0.0309\n0.0321\n\n\n\n\n\nFrom the above results, it verifies the estimates from glm() and optim() are mostly consistent,and this further confirms the right of the MLE.\nThe table demonstrates that both age and age square show a clear inverted-U relationship with the patent counts. The iscustomer has a positive effect which suggests Blueprinty users file more patents, although the size differs slightly between each method. Third, the region effects are small and not statistically significant at the level, while the intercept varies more, mostly may due to optimizer sensitivity. Overall, the firm age and customer status are the most influential predictors.\n\n# counterfactual covariate matrices\nX_0 &lt;- X\nX_0[, \"iscustomer\"] &lt;- 0\nX_1 &lt;- X\nX_1[, \"iscustomer\"] &lt;- 1\n#predict lambda_i for both scenarios\nlambda_0 &lt;- exp(X_0 %*% beta_hat)\nlambda_1 &lt;- exp(X_1 %*% beta_hat)\ndiff &lt;- lambda_1 - lambda_0 #average treatment effect\nate &lt;- mean(diff)\nate\n\n[1] 0.2178843\n\n\nBased on above results, the firms that use Blueprinty’s software are predicted to receive ~0.22 more patents on average than similar firms which do not use the software. This estimated effect is small, but it can support the claim that Blueprinty may help improve the patenting outcomes."
  },
  {
    "objectID": "blog/hw2.html#airbnb-case-study",
    "href": "blog/hw2.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\ntodo: Assume the number of reviews is a good proxy for the number of bookings. Perform some exploratory data analysis to get a feel for the data, handle or drop observations with missing values on relevant variables, build one or more models (e.g., a poisson regression model for the number of bookings as proxied by the number of reviews), and interpret model coefficients to describe variation in the number of reviews as a function of the variables provided.\n\n#exploratory data analysis\nairbnb &lt;- read_csv(\"airbnb.csv\")\n\nNew names:\nRows: 40628 Columns: 14\n── Column specification\n──────────────────────────────────────────────────────── Delimiter: \",\" chr\n(3): last_scraped, host_since, room_type dbl (10): ...1, id, days, bathrooms,\nbedrooms, price, number_of_reviews, rev... lgl (1): instant_bookable\nℹ Use `spec()` to retrieve the full column specification for this data. ℹ\nSpecify the column types or set `show_col_types = FALSE` to quiet this message.\n• `` -&gt; `...1`\n\n#find relevant variables and remove missing values\nairbnb_new &lt;- airbnb %&gt;%\n  select(price, bathrooms, bedrooms, number_of_reviews, review_scores_cleanliness, review_scores_location, review_scores_value, instant_bookable, room_type) %&gt;%\n  drop_na()\nairbnb_new &lt;- airbnb_new %&gt;%\n  mutate(instant_bookable = factor(instant_bookable), room_type = factor(room_type))\nsummary(airbnb_new)\n\n     price           bathrooms        bedrooms      number_of_reviews\n Min.   :   10.0   Min.   :0.000   Min.   : 0.000   Min.   :  1.00   \n 1st Qu.:   70.0   1st Qu.:1.000   1st Qu.: 1.000   1st Qu.:  3.00   \n Median :  103.0   Median :1.000   Median : 1.000   Median :  8.00   \n Mean   :  140.2   Mean   :1.122   Mean   : 1.151   Mean   : 21.17   \n 3rd Qu.:  169.0   3rd Qu.:1.000   3rd Qu.: 1.000   3rd Qu.: 26.00   \n Max.   :10000.0   Max.   :6.000   Max.   :10.000   Max.   :421.00   \n review_scores_cleanliness review_scores_location review_scores_value\n Min.   : 2.000            Min.   : 2.000         Min.   : 2.000     \n 1st Qu.: 9.000            1st Qu.: 9.000         1st Qu.: 9.000     \n Median :10.000            Median :10.000         Median :10.000     \n Mean   : 9.202            Mean   : 9.415         Mean   : 9.334     \n 3rd Qu.:10.000            3rd Qu.:10.000         3rd Qu.:10.000     \n Max.   :10.000            Max.   :10.000         Max.   :10.000     \n instant_bookable           room_type    \n FALSE:24243      Entire home/apt:15543  \n TRUE : 5917      Private room   :13773  \n                  Shared room    :  844  \n                                         \n                                         \n                                         \n\n\n\n#explore data\nhist(airbnb_new$number_of_reviews, breaks=50, col=\"pink\", main= \"Number of Reviews\", xlab=\"Number of Reviews\")\n\n\n\n\n\n\n\nplot(airbnb_new$price, airbnb_new$number_of_reviews,xlab = \"Price\", ylab = \"Number of Reviews\",\n     main = \"Price vs Number of Reviews\", col= \"lightblue\")\n\n\n\n\n\n\n\nplot(airbnb_new$price, log1p(airbnb_new$number_of_reviews),xlab = \"Price\", ylab = \"Log(Number of Reviews+1)\",main = \"Price vs Log Reviews\",col= \"lightgreen\")\n\n\n\n\n\n\n\n\n\nmodel_airbnb &lt;- glm(number_of_reviews ~ price + bathrooms + bedrooms + \n                    review_scores_cleanliness + review_scores_location + \n                    review_scores_value + instant_bookable + room_type,\n                    data = airbnb_new, family = poisson())\n\nsummary(model_airbnb)\n\n\nCall:\nglm(formula = number_of_reviews ~ price + bathrooms + bedrooms + \n    review_scores_cleanliness + review_scores_location + review_scores_value + \n    instant_bookable + room_type, family = poisson(), data = airbnb_new)\n\nCoefficients:\n                            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)                3.572e+00  1.600e-02 223.215  &lt; 2e-16 ***\nprice                     -1.436e-05  8.303e-06  -1.729   0.0838 .  \nbathrooms                 -1.240e-01  3.747e-03 -33.091  &lt; 2e-16 ***\nbedrooms                   7.494e-02  1.988e-03  37.698  &lt; 2e-16 ***\nreview_scores_cleanliness  1.132e-01  1.493e-03  75.821  &lt; 2e-16 ***\nreview_scores_location    -7.680e-02  1.607e-03 -47.796  &lt; 2e-16 ***\nreview_scores_value       -9.153e-02  1.798e-03 -50.902  &lt; 2e-16 ***\ninstant_bookableTRUE       3.344e-01  2.889e-03 115.748  &lt; 2e-16 ***\nroom_typePrivate room     -1.453e-02  2.737e-03  -5.310 1.09e-07 ***\nroom_typeShared room      -2.519e-01  8.618e-03 -29.229  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 961626  on 30159  degrees of freedom\nResidual deviance: 936528  on 30150  degrees of freedom\nAIC: 1058014\n\nNumber of Fisher Scoring iterations: 6\n\n\nFrom the above results, I notice that listings with more bedrooms, higher cleanliness scores, and instant bookable status tend to have more reviews. The bathrooms and higher prices are related to fewer reviews. In addition, for the shared rooms, it has fewer reviews than the entire homes, but the private rooms show negative effect. The cleanliness is the most influential review score here; location and value have unexpected negative coefficients; this may be due to bias existing in the sample. To sum up, from the exploration of the data, it shows the convenience, cleanliness, and room type are key drivers of booking activity."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Boning Yang",
    "section": "",
    "text": "Here’s a paragraph about me :)"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "blog/project1/index.html",
    "href": "blog/project1/index.html",
    "title": "This is Project 1",
    "section": "",
    "text": "I cleaned some data\n\n\n\nI analyzed the data\n#| message: false\nlibrary(tidyverse) mtcars |&gt; ggplot(aes(x = wt, y = mpg)) + geom_point()"
  },
  {
    "objectID": "blog/project1/index.html#section-1-data",
    "href": "blog/project1/index.html#section-1-data",
    "title": "This is Project 1",
    "section": "",
    "text": "I cleaned some data"
  },
  {
    "objectID": "blog/project1/index.html#section-2-analysis",
    "href": "blog/project1/index.html#section-2-analysis",
    "title": "This is Project 1",
    "section": "",
    "text": "I analyzed the data\n#| message: false\nlibrary(tidyverse) mtcars |&gt; ggplot(aes(x = wt, y = mpg)) + geom_point()"
  },
  {
    "objectID": "blog/project2/index.html",
    "href": "blog/project2/index.html",
    "title": "This is Project 2",
    "section": "",
    "text": "I cleaned some data\n\n\n\nI analyzed the data\n#| message: false\nlibrary(tidyverse) mtcars |&gt; ggplot(aes(x = wt, y = mpg)) + geom_point()"
  },
  {
    "objectID": "blog/project2/index.html#section-1-data",
    "href": "blog/project2/index.html#section-1-data",
    "title": "This is Project 2",
    "section": "",
    "text": "I cleaned some data"
  },
  {
    "objectID": "blog/project2/index.html#section-2-analysis",
    "href": "blog/project2/index.html#section-2-analysis",
    "title": "This is Project 2",
    "section": "",
    "text": "I analyzed the data\n#| message: false\nlibrary(tidyverse) mtcars |&gt; ggplot(aes(x = wt, y = mpg)) + geom_point()"
  }
]